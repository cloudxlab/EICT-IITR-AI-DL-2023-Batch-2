{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chapter 18 – Reinforcement Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn ≥0.20 and TensorFlow ≥2.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# To get smooth animations\n",
    "import matplotlib.animation as animation\n",
    "mpl.rc('animation', html='jshtml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to OpenAI gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will be using [OpenAI gym](https://gym.openai.com/), a great toolkit for developing and comparing Reinforcement Learning algorithms. It provides many environments for your learning *agents* to interact with. Let's start by importing `gym`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's list all the available environments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([EnvSpec(Copy-v0), EnvSpec(RepeatCopy-v0), EnvSpec(ReversedAddition-v0), EnvSpec(ReversedAddition3-v0), EnvSpec(DuplicatedInput-v0), EnvSpec(Reverse-v0), EnvSpec(CartPole-v0), EnvSpec(CartPole-v1), EnvSpec(MountainCar-v0), EnvSpec(MountainCarContinuous-v0), EnvSpec(Pendulum-v0), EnvSpec(Acrobot-v1), EnvSpec(LunarLander-v2), EnvSpec(LunarLanderContinuous-v2), EnvSpec(BipedalWalker-v3), EnvSpec(BipedalWalkerHardcore-v3), EnvSpec(CarRacing-v0), EnvSpec(Blackjack-v0), EnvSpec(KellyCoinflip-v0), EnvSpec(KellyCoinflipGeneralized-v0), EnvSpec(FrozenLake-v0), EnvSpec(FrozenLake8x8-v0), EnvSpec(CliffWalking-v0), EnvSpec(NChain-v0), EnvSpec(Roulette-v0), EnvSpec(Taxi-v3), EnvSpec(GuessingGame-v0), EnvSpec(HotterColder-v0), EnvSpec(Reacher-v2), EnvSpec(Pusher-v2), EnvSpec(Thrower-v2), EnvSpec(Striker-v2), EnvSpec(InvertedPendulum-v2), EnvSpec(InvertedDoublePendulum-v2), EnvSpec(HalfCheetah-v2), EnvSpec(HalfCheetah-v3), EnvSpec(Hopper-v2), EnvSpec(Hopper-v3), EnvSpec(Swimmer-v2), EnvSpec(Swimmer-v3), EnvSpec(Walker2d-v2), EnvSpec(Walker2d-v3), EnvSpec(Ant-v2), EnvSpec(Ant-v3), EnvSpec(Humanoid-v2), EnvSpec(Humanoid-v3), EnvSpec(HumanoidStandup-v2), EnvSpec(FetchSlide-v1), EnvSpec(FetchPickAndPlace-v1), EnvSpec(FetchReach-v1), EnvSpec(FetchPush-v1), EnvSpec(HandReach-v0), EnvSpec(HandManipulateBlockRotateZ-v0), EnvSpec(HandManipulateBlockRotateZTouchSensors-v0), EnvSpec(HandManipulateBlockRotateZTouchSensors-v1), EnvSpec(HandManipulateBlockRotateParallel-v0), EnvSpec(HandManipulateBlockRotateParallelTouchSensors-v0), EnvSpec(HandManipulateBlockRotateParallelTouchSensors-v1), EnvSpec(HandManipulateBlockRotateXYZ-v0), EnvSpec(HandManipulateBlockRotateXYZTouchSensors-v0), EnvSpec(HandManipulateBlockRotateXYZTouchSensors-v1), EnvSpec(HandManipulateBlockFull-v0), EnvSpec(HandManipulateBlock-v0), EnvSpec(HandManipulateBlockTouchSensors-v0), EnvSpec(HandManipulateBlockTouchSensors-v1), EnvSpec(HandManipulateEggRotate-v0), EnvSpec(HandManipulateEggRotateTouchSensors-v0), EnvSpec(HandManipulateEggRotateTouchSensors-v1), EnvSpec(HandManipulateEggFull-v0), EnvSpec(HandManipulateEgg-v0), EnvSpec(HandManipulateEggTouchSensors-v0), EnvSpec(HandManipulateEggTouchSensors-v1), EnvSpec(HandManipulatePenRotate-v0), EnvSpec(HandManipulatePenRotateTouchSensors-v0), EnvSpec(HandManipulatePenRotateTouchSensors-v1), EnvSpec(HandManipulatePenFull-v0), EnvSpec(HandManipulatePen-v0), EnvSpec(HandManipulatePenTouchSensors-v0), EnvSpec(HandManipulatePenTouchSensors-v1), EnvSpec(FetchSlideDense-v1), EnvSpec(FetchPickAndPlaceDense-v1), EnvSpec(FetchReachDense-v1), EnvSpec(FetchPushDense-v1), EnvSpec(HandReachDense-v0), EnvSpec(HandManipulateBlockRotateZDense-v0), EnvSpec(HandManipulateBlockRotateZTouchSensorsDense-v0), EnvSpec(HandManipulateBlockRotateZTouchSensorsDense-v1), EnvSpec(HandManipulateBlockRotateParallelDense-v0), EnvSpec(HandManipulateBlockRotateParallelTouchSensorsDense-v0), EnvSpec(HandManipulateBlockRotateParallelTouchSensorsDense-v1), EnvSpec(HandManipulateBlockRotateXYZDense-v0), EnvSpec(HandManipulateBlockRotateXYZTouchSensorsDense-v0), EnvSpec(HandManipulateBlockRotateXYZTouchSensorsDense-v1), EnvSpec(HandManipulateBlockFullDense-v0), EnvSpec(HandManipulateBlockDense-v0), EnvSpec(HandManipulateBlockTouchSensorsDense-v0), EnvSpec(HandManipulateBlockTouchSensorsDense-v1), EnvSpec(HandManipulateEggRotateDense-v0), EnvSpec(HandManipulateEggRotateTouchSensorsDense-v0), EnvSpec(HandManipulateEggRotateTouchSensorsDense-v1), EnvSpec(HandManipulateEggFullDense-v0), EnvSpec(HandManipulateEggDense-v0), EnvSpec(HandManipulateEggTouchSensorsDense-v0), EnvSpec(HandManipulateEggTouchSensorsDense-v1), EnvSpec(HandManipulatePenRotateDense-v0), EnvSpec(HandManipulatePenRotateTouchSensorsDense-v0), EnvSpec(HandManipulatePenRotateTouchSensorsDense-v1), EnvSpec(HandManipulatePenFullDense-v0), EnvSpec(HandManipulatePenDense-v0), EnvSpec(HandManipulatePenTouchSensorsDense-v0), EnvSpec(HandManipulatePenTouchSensorsDense-v1), EnvSpec(Adventure-v0), EnvSpec(Adventure-v4), EnvSpec(AdventureDeterministic-v0), EnvSpec(AdventureDeterministic-v4), EnvSpec(AdventureNoFrameskip-v0), EnvSpec(AdventureNoFrameskip-v4), EnvSpec(Adventure-ram-v0), EnvSpec(Adventure-ram-v4), EnvSpec(Adventure-ramDeterministic-v0), EnvSpec(Adventure-ramDeterministic-v4), EnvSpec(Adventure-ramNoFrameskip-v0), EnvSpec(Adventure-ramNoFrameskip-v4), EnvSpec(AirRaid-v0), EnvSpec(AirRaid-v4), EnvSpec(AirRaidDeterministic-v0), EnvSpec(AirRaidDeterministic-v4), EnvSpec(AirRaidNoFrameskip-v0), EnvSpec(AirRaidNoFrameskip-v4), EnvSpec(AirRaid-ram-v0), EnvSpec(AirRaid-ram-v4), EnvSpec(AirRaid-ramDeterministic-v0), EnvSpec(AirRaid-ramDeterministic-v4), EnvSpec(AirRaid-ramNoFrameskip-v0), EnvSpec(AirRaid-ramNoFrameskip-v4), EnvSpec(Alien-v0), EnvSpec(Alien-v4), EnvSpec(AlienDeterministic-v0), EnvSpec(AlienDeterministic-v4), EnvSpec(AlienNoFrameskip-v0), EnvSpec(AlienNoFrameskip-v4), EnvSpec(Alien-ram-v0), EnvSpec(Alien-ram-v4), EnvSpec(Alien-ramDeterministic-v0), EnvSpec(Alien-ramDeterministic-v4), EnvSpec(Alien-ramNoFrameskip-v0), EnvSpec(Alien-ramNoFrameskip-v4), EnvSpec(Amidar-v0), EnvSpec(Amidar-v4), EnvSpec(AmidarDeterministic-v0), EnvSpec(AmidarDeterministic-v4), EnvSpec(AmidarNoFrameskip-v0), EnvSpec(AmidarNoFrameskip-v4), EnvSpec(Amidar-ram-v0), EnvSpec(Amidar-ram-v4), EnvSpec(Amidar-ramDeterministic-v0), EnvSpec(Amidar-ramDeterministic-v4), EnvSpec(Amidar-ramNoFrameskip-v0), EnvSpec(Amidar-ramNoFrameskip-v4), EnvSpec(Assault-v0), EnvSpec(Assault-v4), EnvSpec(AssaultDeterministic-v0), EnvSpec(AssaultDeterministic-v4), EnvSpec(AssaultNoFrameskip-v0), EnvSpec(AssaultNoFrameskip-v4), EnvSpec(Assault-ram-v0), EnvSpec(Assault-ram-v4), EnvSpec(Assault-ramDeterministic-v0), EnvSpec(Assault-ramDeterministic-v4), EnvSpec(Assault-ramNoFrameskip-v0), EnvSpec(Assault-ramNoFrameskip-v4), EnvSpec(Asterix-v0), EnvSpec(Asterix-v4), EnvSpec(AsterixDeterministic-v0), EnvSpec(AsterixDeterministic-v4), EnvSpec(AsterixNoFrameskip-v0), EnvSpec(AsterixNoFrameskip-v4), EnvSpec(Asterix-ram-v0), EnvSpec(Asterix-ram-v4), EnvSpec(Asterix-ramDeterministic-v0), EnvSpec(Asterix-ramDeterministic-v4), EnvSpec(Asterix-ramNoFrameskip-v0), EnvSpec(Asterix-ramNoFrameskip-v4), EnvSpec(Asteroids-v0), EnvSpec(Asteroids-v4), EnvSpec(AsteroidsDeterministic-v0), EnvSpec(AsteroidsDeterministic-v4), EnvSpec(AsteroidsNoFrameskip-v0), EnvSpec(AsteroidsNoFrameskip-v4), EnvSpec(Asteroids-ram-v0), EnvSpec(Asteroids-ram-v4), EnvSpec(Asteroids-ramDeterministic-v0), EnvSpec(Asteroids-ramDeterministic-v4), EnvSpec(Asteroids-ramNoFrameskip-v0), EnvSpec(Asteroids-ramNoFrameskip-v4), EnvSpec(Atlantis-v0), EnvSpec(Atlantis-v4), EnvSpec(AtlantisDeterministic-v0), EnvSpec(AtlantisDeterministic-v4), EnvSpec(AtlantisNoFrameskip-v0), EnvSpec(AtlantisNoFrameskip-v4), EnvSpec(Atlantis-ram-v0), EnvSpec(Atlantis-ram-v4), EnvSpec(Atlantis-ramDeterministic-v0), EnvSpec(Atlantis-ramDeterministic-v4), EnvSpec(Atlantis-ramNoFrameskip-v0), EnvSpec(Atlantis-ramNoFrameskip-v4), EnvSpec(BankHeist-v0), EnvSpec(BankHeist-v4), EnvSpec(BankHeistDeterministic-v0), EnvSpec(BankHeistDeterministic-v4), EnvSpec(BankHeistNoFrameskip-v0), EnvSpec(BankHeistNoFrameskip-v4), EnvSpec(BankHeist-ram-v0), EnvSpec(BankHeist-ram-v4), EnvSpec(BankHeist-ramDeterministic-v0), EnvSpec(BankHeist-ramDeterministic-v4), EnvSpec(BankHeist-ramNoFrameskip-v0), EnvSpec(BankHeist-ramNoFrameskip-v4), EnvSpec(BattleZone-v0), EnvSpec(BattleZone-v4), EnvSpec(BattleZoneDeterministic-v0), EnvSpec(BattleZoneDeterministic-v4), EnvSpec(BattleZoneNoFrameskip-v0), EnvSpec(BattleZoneNoFrameskip-v4), EnvSpec(BattleZone-ram-v0), EnvSpec(BattleZone-ram-v4), EnvSpec(BattleZone-ramDeterministic-v0), EnvSpec(BattleZone-ramDeterministic-v4), EnvSpec(BattleZone-ramNoFrameskip-v0), EnvSpec(BattleZone-ramNoFrameskip-v4), EnvSpec(BeamRider-v0), EnvSpec(BeamRider-v4), EnvSpec(BeamRiderDeterministic-v0), EnvSpec(BeamRiderDeterministic-v4), EnvSpec(BeamRiderNoFrameskip-v0), EnvSpec(BeamRiderNoFrameskip-v4), EnvSpec(BeamRider-ram-v0), EnvSpec(BeamRider-ram-v4), EnvSpec(BeamRider-ramDeterministic-v0), EnvSpec(BeamRider-ramDeterministic-v4), EnvSpec(BeamRider-ramNoFrameskip-v0), EnvSpec(BeamRider-ramNoFrameskip-v4), EnvSpec(Berzerk-v0), EnvSpec(Berzerk-v4), EnvSpec(BerzerkDeterministic-v0), EnvSpec(BerzerkDeterministic-v4), EnvSpec(BerzerkNoFrameskip-v0), EnvSpec(BerzerkNoFrameskip-v4), EnvSpec(Berzerk-ram-v0), EnvSpec(Berzerk-ram-v4), EnvSpec(Berzerk-ramDeterministic-v0), EnvSpec(Berzerk-ramDeterministic-v4), EnvSpec(Berzerk-ramNoFrameskip-v0), EnvSpec(Berzerk-ramNoFrameskip-v4), EnvSpec(Bowling-v0), EnvSpec(Bowling-v4), EnvSpec(BowlingDeterministic-v0), EnvSpec(BowlingDeterministic-v4), EnvSpec(BowlingNoFrameskip-v0), EnvSpec(BowlingNoFrameskip-v4), EnvSpec(Bowling-ram-v0), EnvSpec(Bowling-ram-v4), EnvSpec(Bowling-ramDeterministic-v0), EnvSpec(Bowling-ramDeterministic-v4), EnvSpec(Bowling-ramNoFrameskip-v0), EnvSpec(Bowling-ramNoFrameskip-v4), EnvSpec(Boxing-v0), EnvSpec(Boxing-v4), EnvSpec(BoxingDeterministic-v0), EnvSpec(BoxingDeterministic-v4), EnvSpec(BoxingNoFrameskip-v0), EnvSpec(BoxingNoFrameskip-v4), EnvSpec(Boxing-ram-v0), EnvSpec(Boxing-ram-v4), EnvSpec(Boxing-ramDeterministic-v0), EnvSpec(Boxing-ramDeterministic-v4), EnvSpec(Boxing-ramNoFrameskip-v0), EnvSpec(Boxing-ramNoFrameskip-v4), EnvSpec(Breakout-v0), EnvSpec(Breakout-v4), EnvSpec(BreakoutDeterministic-v0), EnvSpec(BreakoutDeterministic-v4), EnvSpec(BreakoutNoFrameskip-v0), EnvSpec(BreakoutNoFrameskip-v4), EnvSpec(Breakout-ram-v0), EnvSpec(Breakout-ram-v4), EnvSpec(Breakout-ramDeterministic-v0), EnvSpec(Breakout-ramDeterministic-v4), EnvSpec(Breakout-ramNoFrameskip-v0), EnvSpec(Breakout-ramNoFrameskip-v4), EnvSpec(Carnival-v0), EnvSpec(Carnival-v4), EnvSpec(CarnivalDeterministic-v0), EnvSpec(CarnivalDeterministic-v4), EnvSpec(CarnivalNoFrameskip-v0), EnvSpec(CarnivalNoFrameskip-v4), EnvSpec(Carnival-ram-v0), EnvSpec(Carnival-ram-v4), EnvSpec(Carnival-ramDeterministic-v0), EnvSpec(Carnival-ramDeterministic-v4), EnvSpec(Carnival-ramNoFrameskip-v0), EnvSpec(Carnival-ramNoFrameskip-v4), EnvSpec(Centipede-v0), EnvSpec(Centipede-v4), EnvSpec(CentipedeDeterministic-v0), EnvSpec(CentipedeDeterministic-v4), EnvSpec(CentipedeNoFrameskip-v0), EnvSpec(CentipedeNoFrameskip-v4), EnvSpec(Centipede-ram-v0), EnvSpec(Centipede-ram-v4), EnvSpec(Centipede-ramDeterministic-v0), EnvSpec(Centipede-ramDeterministic-v4), EnvSpec(Centipede-ramNoFrameskip-v0), EnvSpec(Centipede-ramNoFrameskip-v4), EnvSpec(ChopperCommand-v0), EnvSpec(ChopperCommand-v4), EnvSpec(ChopperCommandDeterministic-v0), EnvSpec(ChopperCommandDeterministic-v4), EnvSpec(ChopperCommandNoFrameskip-v0), EnvSpec(ChopperCommandNoFrameskip-v4), EnvSpec(ChopperCommand-ram-v0), EnvSpec(ChopperCommand-ram-v4), EnvSpec(ChopperCommand-ramDeterministic-v0), EnvSpec(ChopperCommand-ramDeterministic-v4), EnvSpec(ChopperCommand-ramNoFrameskip-v0), EnvSpec(ChopperCommand-ramNoFrameskip-v4), EnvSpec(CrazyClimber-v0), EnvSpec(CrazyClimber-v4), EnvSpec(CrazyClimberDeterministic-v0), EnvSpec(CrazyClimberDeterministic-v4), EnvSpec(CrazyClimberNoFrameskip-v0), EnvSpec(CrazyClimberNoFrameskip-v4), EnvSpec(CrazyClimber-ram-v0), EnvSpec(CrazyClimber-ram-v4), EnvSpec(CrazyClimber-ramDeterministic-v0), EnvSpec(CrazyClimber-ramDeterministic-v4), EnvSpec(CrazyClimber-ramNoFrameskip-v0), EnvSpec(CrazyClimber-ramNoFrameskip-v4), EnvSpec(Defender-v0), EnvSpec(Defender-v4), EnvSpec(DefenderDeterministic-v0), EnvSpec(DefenderDeterministic-v4), EnvSpec(DefenderNoFrameskip-v0), EnvSpec(DefenderNoFrameskip-v4), EnvSpec(Defender-ram-v0), EnvSpec(Defender-ram-v4), EnvSpec(Defender-ramDeterministic-v0), EnvSpec(Defender-ramDeterministic-v4), EnvSpec(Defender-ramNoFrameskip-v0), EnvSpec(Defender-ramNoFrameskip-v4), EnvSpec(DemonAttack-v0), EnvSpec(DemonAttack-v4), EnvSpec(DemonAttackDeterministic-v0), EnvSpec(DemonAttackDeterministic-v4), EnvSpec(DemonAttackNoFrameskip-v0), EnvSpec(DemonAttackNoFrameskip-v4), EnvSpec(DemonAttack-ram-v0), EnvSpec(DemonAttack-ram-v4), EnvSpec(DemonAttack-ramDeterministic-v0), EnvSpec(DemonAttack-ramDeterministic-v4), EnvSpec(DemonAttack-ramNoFrameskip-v0), EnvSpec(DemonAttack-ramNoFrameskip-v4), EnvSpec(DoubleDunk-v0), EnvSpec(DoubleDunk-v4), EnvSpec(DoubleDunkDeterministic-v0), EnvSpec(DoubleDunkDeterministic-v4), EnvSpec(DoubleDunkNoFrameskip-v0), EnvSpec(DoubleDunkNoFrameskip-v4), EnvSpec(DoubleDunk-ram-v0), EnvSpec(DoubleDunk-ram-v4), EnvSpec(DoubleDunk-ramDeterministic-v0), EnvSpec(DoubleDunk-ramDeterministic-v4), EnvSpec(DoubleDunk-ramNoFrameskip-v0), EnvSpec(DoubleDunk-ramNoFrameskip-v4), EnvSpec(ElevatorAction-v0), EnvSpec(ElevatorAction-v4), EnvSpec(ElevatorActionDeterministic-v0), EnvSpec(ElevatorActionDeterministic-v4), EnvSpec(ElevatorActionNoFrameskip-v0), EnvSpec(ElevatorActionNoFrameskip-v4), EnvSpec(ElevatorAction-ram-v0), EnvSpec(ElevatorAction-ram-v4), EnvSpec(ElevatorAction-ramDeterministic-v0), EnvSpec(ElevatorAction-ramDeterministic-v4), EnvSpec(ElevatorAction-ramNoFrameskip-v0), EnvSpec(ElevatorAction-ramNoFrameskip-v4), EnvSpec(Enduro-v0), EnvSpec(Enduro-v4), EnvSpec(EnduroDeterministic-v0), EnvSpec(EnduroDeterministic-v4), EnvSpec(EnduroNoFrameskip-v0), EnvSpec(EnduroNoFrameskip-v4), EnvSpec(Enduro-ram-v0), EnvSpec(Enduro-ram-v4), EnvSpec(Enduro-ramDeterministic-v0), EnvSpec(Enduro-ramDeterministic-v4), EnvSpec(Enduro-ramNoFrameskip-v0), EnvSpec(Enduro-ramNoFrameskip-v4), EnvSpec(FishingDerby-v0), EnvSpec(FishingDerby-v4), EnvSpec(FishingDerbyDeterministic-v0), EnvSpec(FishingDerbyDeterministic-v4), EnvSpec(FishingDerbyNoFrameskip-v0), EnvSpec(FishingDerbyNoFrameskip-v4), EnvSpec(FishingDerby-ram-v0), EnvSpec(FishingDerby-ram-v4), EnvSpec(FishingDerby-ramDeterministic-v0), EnvSpec(FishingDerby-ramDeterministic-v4), EnvSpec(FishingDerby-ramNoFrameskip-v0), EnvSpec(FishingDerby-ramNoFrameskip-v4), EnvSpec(Freeway-v0), EnvSpec(Freeway-v4), EnvSpec(FreewayDeterministic-v0), EnvSpec(FreewayDeterministic-v4), EnvSpec(FreewayNoFrameskip-v0), EnvSpec(FreewayNoFrameskip-v4), EnvSpec(Freeway-ram-v0), EnvSpec(Freeway-ram-v4), EnvSpec(Freeway-ramDeterministic-v0), EnvSpec(Freeway-ramDeterministic-v4), EnvSpec(Freeway-ramNoFrameskip-v0), EnvSpec(Freeway-ramNoFrameskip-v4), EnvSpec(Frostbite-v0), EnvSpec(Frostbite-v4), EnvSpec(FrostbiteDeterministic-v0), EnvSpec(FrostbiteDeterministic-v4), EnvSpec(FrostbiteNoFrameskip-v0), EnvSpec(FrostbiteNoFrameskip-v4), EnvSpec(Frostbite-ram-v0), EnvSpec(Frostbite-ram-v4), EnvSpec(Frostbite-ramDeterministic-v0), EnvSpec(Frostbite-ramDeterministic-v4), EnvSpec(Frostbite-ramNoFrameskip-v0), EnvSpec(Frostbite-ramNoFrameskip-v4), EnvSpec(Gopher-v0), EnvSpec(Gopher-v4), EnvSpec(GopherDeterministic-v0), EnvSpec(GopherDeterministic-v4), EnvSpec(GopherNoFrameskip-v0), EnvSpec(GopherNoFrameskip-v4), EnvSpec(Gopher-ram-v0), EnvSpec(Gopher-ram-v4), EnvSpec(Gopher-ramDeterministic-v0), EnvSpec(Gopher-ramDeterministic-v4), EnvSpec(Gopher-ramNoFrameskip-v0), EnvSpec(Gopher-ramNoFrameskip-v4), EnvSpec(Gravitar-v0), EnvSpec(Gravitar-v4), EnvSpec(GravitarDeterministic-v0), EnvSpec(GravitarDeterministic-v4), EnvSpec(GravitarNoFrameskip-v0), EnvSpec(GravitarNoFrameskip-v4), EnvSpec(Gravitar-ram-v0), EnvSpec(Gravitar-ram-v4), EnvSpec(Gravitar-ramDeterministic-v0), EnvSpec(Gravitar-ramDeterministic-v4), EnvSpec(Gravitar-ramNoFrameskip-v0), EnvSpec(Gravitar-ramNoFrameskip-v4), EnvSpec(Hero-v0), EnvSpec(Hero-v4), EnvSpec(HeroDeterministic-v0), EnvSpec(HeroDeterministic-v4), EnvSpec(HeroNoFrameskip-v0), EnvSpec(HeroNoFrameskip-v4), EnvSpec(Hero-ram-v0), EnvSpec(Hero-ram-v4), EnvSpec(Hero-ramDeterministic-v0), EnvSpec(Hero-ramDeterministic-v4), EnvSpec(Hero-ramNoFrameskip-v0), EnvSpec(Hero-ramNoFrameskip-v4), EnvSpec(IceHockey-v0), EnvSpec(IceHockey-v4), EnvSpec(IceHockeyDeterministic-v0), EnvSpec(IceHockeyDeterministic-v4), EnvSpec(IceHockeyNoFrameskip-v0), EnvSpec(IceHockeyNoFrameskip-v4), EnvSpec(IceHockey-ram-v0), EnvSpec(IceHockey-ram-v4), EnvSpec(IceHockey-ramDeterministic-v0), EnvSpec(IceHockey-ramDeterministic-v4), EnvSpec(IceHockey-ramNoFrameskip-v0), EnvSpec(IceHockey-ramNoFrameskip-v4), EnvSpec(Jamesbond-v0), EnvSpec(Jamesbond-v4), EnvSpec(JamesbondDeterministic-v0), EnvSpec(JamesbondDeterministic-v4), EnvSpec(JamesbondNoFrameskip-v0), EnvSpec(JamesbondNoFrameskip-v4), EnvSpec(Jamesbond-ram-v0), EnvSpec(Jamesbond-ram-v4), EnvSpec(Jamesbond-ramDeterministic-v0), EnvSpec(Jamesbond-ramDeterministic-v4), EnvSpec(Jamesbond-ramNoFrameskip-v0), EnvSpec(Jamesbond-ramNoFrameskip-v4), EnvSpec(JourneyEscape-v0), EnvSpec(JourneyEscape-v4), EnvSpec(JourneyEscapeDeterministic-v0), EnvSpec(JourneyEscapeDeterministic-v4), EnvSpec(JourneyEscapeNoFrameskip-v0), EnvSpec(JourneyEscapeNoFrameskip-v4), EnvSpec(JourneyEscape-ram-v0), EnvSpec(JourneyEscape-ram-v4), EnvSpec(JourneyEscape-ramDeterministic-v0), EnvSpec(JourneyEscape-ramDeterministic-v4), EnvSpec(JourneyEscape-ramNoFrameskip-v0), EnvSpec(JourneyEscape-ramNoFrameskip-v4), EnvSpec(Kangaroo-v0), EnvSpec(Kangaroo-v4), EnvSpec(KangarooDeterministic-v0), EnvSpec(KangarooDeterministic-v4), EnvSpec(KangarooNoFrameskip-v0), EnvSpec(KangarooNoFrameskip-v4), EnvSpec(Kangaroo-ram-v0), EnvSpec(Kangaroo-ram-v4), EnvSpec(Kangaroo-ramDeterministic-v0), EnvSpec(Kangaroo-ramDeterministic-v4), EnvSpec(Kangaroo-ramNoFrameskip-v0), EnvSpec(Kangaroo-ramNoFrameskip-v4), EnvSpec(Krull-v0), EnvSpec(Krull-v4), EnvSpec(KrullDeterministic-v0), EnvSpec(KrullDeterministic-v4), EnvSpec(KrullNoFrameskip-v0), EnvSpec(KrullNoFrameskip-v4), EnvSpec(Krull-ram-v0), EnvSpec(Krull-ram-v4), EnvSpec(Krull-ramDeterministic-v0), EnvSpec(Krull-ramDeterministic-v4), EnvSpec(Krull-ramNoFrameskip-v0), EnvSpec(Krull-ramNoFrameskip-v4), EnvSpec(KungFuMaster-v0), EnvSpec(KungFuMaster-v4), EnvSpec(KungFuMasterDeterministic-v0), EnvSpec(KungFuMasterDeterministic-v4), EnvSpec(KungFuMasterNoFrameskip-v0), EnvSpec(KungFuMasterNoFrameskip-v4), EnvSpec(KungFuMaster-ram-v0), EnvSpec(KungFuMaster-ram-v4), EnvSpec(KungFuMaster-ramDeterministic-v0), EnvSpec(KungFuMaster-ramDeterministic-v4), EnvSpec(KungFuMaster-ramNoFrameskip-v0), EnvSpec(KungFuMaster-ramNoFrameskip-v4), EnvSpec(MontezumaRevenge-v0), EnvSpec(MontezumaRevenge-v4), EnvSpec(MontezumaRevengeDeterministic-v0), EnvSpec(MontezumaRevengeDeterministic-v4), EnvSpec(MontezumaRevengeNoFrameskip-v0), EnvSpec(MontezumaRevengeNoFrameskip-v4), EnvSpec(MontezumaRevenge-ram-v0), EnvSpec(MontezumaRevenge-ram-v4), EnvSpec(MontezumaRevenge-ramDeterministic-v0), EnvSpec(MontezumaRevenge-ramDeterministic-v4), EnvSpec(MontezumaRevenge-ramNoFrameskip-v0), EnvSpec(MontezumaRevenge-ramNoFrameskip-v4), EnvSpec(MsPacman-v0), EnvSpec(MsPacman-v4), EnvSpec(MsPacmanDeterministic-v0), EnvSpec(MsPacmanDeterministic-v4), EnvSpec(MsPacmanNoFrameskip-v0), EnvSpec(MsPacmanNoFrameskip-v4), EnvSpec(MsPacman-ram-v0), EnvSpec(MsPacman-ram-v4), EnvSpec(MsPacman-ramDeterministic-v0), EnvSpec(MsPacman-ramDeterministic-v4), EnvSpec(MsPacman-ramNoFrameskip-v0), EnvSpec(MsPacman-ramNoFrameskip-v4), EnvSpec(NameThisGame-v0), EnvSpec(NameThisGame-v4), EnvSpec(NameThisGameDeterministic-v0), EnvSpec(NameThisGameDeterministic-v4), EnvSpec(NameThisGameNoFrameskip-v0), EnvSpec(NameThisGameNoFrameskip-v4), EnvSpec(NameThisGame-ram-v0), EnvSpec(NameThisGame-ram-v4), EnvSpec(NameThisGame-ramDeterministic-v0), EnvSpec(NameThisGame-ramDeterministic-v4), EnvSpec(NameThisGame-ramNoFrameskip-v0), EnvSpec(NameThisGame-ramNoFrameskip-v4), EnvSpec(Phoenix-v0), EnvSpec(Phoenix-v4), EnvSpec(PhoenixDeterministic-v0), EnvSpec(PhoenixDeterministic-v4), EnvSpec(PhoenixNoFrameskip-v0), EnvSpec(PhoenixNoFrameskip-v4), EnvSpec(Phoenix-ram-v0), EnvSpec(Phoenix-ram-v4), EnvSpec(Phoenix-ramDeterministic-v0), EnvSpec(Phoenix-ramDeterministic-v4), EnvSpec(Phoenix-ramNoFrameskip-v0), EnvSpec(Phoenix-ramNoFrameskip-v4), EnvSpec(Pitfall-v0), EnvSpec(Pitfall-v4), EnvSpec(PitfallDeterministic-v0), EnvSpec(PitfallDeterministic-v4), EnvSpec(PitfallNoFrameskip-v0), EnvSpec(PitfallNoFrameskip-v4), EnvSpec(Pitfall-ram-v0), EnvSpec(Pitfall-ram-v4), EnvSpec(Pitfall-ramDeterministic-v0), EnvSpec(Pitfall-ramDeterministic-v4), EnvSpec(Pitfall-ramNoFrameskip-v0), EnvSpec(Pitfall-ramNoFrameskip-v4), EnvSpec(Pong-v0), EnvSpec(Pong-v4), EnvSpec(PongDeterministic-v0), EnvSpec(PongDeterministic-v4), EnvSpec(PongNoFrameskip-v0), EnvSpec(PongNoFrameskip-v4), EnvSpec(Pong-ram-v0), EnvSpec(Pong-ram-v4), EnvSpec(Pong-ramDeterministic-v0), EnvSpec(Pong-ramDeterministic-v4), EnvSpec(Pong-ramNoFrameskip-v0), EnvSpec(Pong-ramNoFrameskip-v4), EnvSpec(Pooyan-v0), EnvSpec(Pooyan-v4), EnvSpec(PooyanDeterministic-v0), EnvSpec(PooyanDeterministic-v4), EnvSpec(PooyanNoFrameskip-v0), EnvSpec(PooyanNoFrameskip-v4), EnvSpec(Pooyan-ram-v0), EnvSpec(Pooyan-ram-v4), EnvSpec(Pooyan-ramDeterministic-v0), EnvSpec(Pooyan-ramDeterministic-v4), EnvSpec(Pooyan-ramNoFrameskip-v0), EnvSpec(Pooyan-ramNoFrameskip-v4), EnvSpec(PrivateEye-v0), EnvSpec(PrivateEye-v4), EnvSpec(PrivateEyeDeterministic-v0), EnvSpec(PrivateEyeDeterministic-v4), EnvSpec(PrivateEyeNoFrameskip-v0), EnvSpec(PrivateEyeNoFrameskip-v4), EnvSpec(PrivateEye-ram-v0), EnvSpec(PrivateEye-ram-v4), EnvSpec(PrivateEye-ramDeterministic-v0), EnvSpec(PrivateEye-ramDeterministic-v4), EnvSpec(PrivateEye-ramNoFrameskip-v0), EnvSpec(PrivateEye-ramNoFrameskip-v4), EnvSpec(Qbert-v0), EnvSpec(Qbert-v4), EnvSpec(QbertDeterministic-v0), EnvSpec(QbertDeterministic-v4), EnvSpec(QbertNoFrameskip-v0), EnvSpec(QbertNoFrameskip-v4), EnvSpec(Qbert-ram-v0), EnvSpec(Qbert-ram-v4), EnvSpec(Qbert-ramDeterministic-v0), EnvSpec(Qbert-ramDeterministic-v4), EnvSpec(Qbert-ramNoFrameskip-v0), EnvSpec(Qbert-ramNoFrameskip-v4), EnvSpec(Riverraid-v0), EnvSpec(Riverraid-v4), EnvSpec(RiverraidDeterministic-v0), EnvSpec(RiverraidDeterministic-v4), EnvSpec(RiverraidNoFrameskip-v0), EnvSpec(RiverraidNoFrameskip-v4), EnvSpec(Riverraid-ram-v0), EnvSpec(Riverraid-ram-v4), EnvSpec(Riverraid-ramDeterministic-v0), EnvSpec(Riverraid-ramDeterministic-v4), EnvSpec(Riverraid-ramNoFrameskip-v0), EnvSpec(Riverraid-ramNoFrameskip-v4), EnvSpec(RoadRunner-v0), EnvSpec(RoadRunner-v4), EnvSpec(RoadRunnerDeterministic-v0), EnvSpec(RoadRunnerDeterministic-v4), EnvSpec(RoadRunnerNoFrameskip-v0), EnvSpec(RoadRunnerNoFrameskip-v4), EnvSpec(RoadRunner-ram-v0), EnvSpec(RoadRunner-ram-v4), EnvSpec(RoadRunner-ramDeterministic-v0), EnvSpec(RoadRunner-ramDeterministic-v4), EnvSpec(RoadRunner-ramNoFrameskip-v0), EnvSpec(RoadRunner-ramNoFrameskip-v4), EnvSpec(Robotank-v0), EnvSpec(Robotank-v4), EnvSpec(RobotankDeterministic-v0), EnvSpec(RobotankDeterministic-v4), EnvSpec(RobotankNoFrameskip-v0), EnvSpec(RobotankNoFrameskip-v4), EnvSpec(Robotank-ram-v0), EnvSpec(Robotank-ram-v4), EnvSpec(Robotank-ramDeterministic-v0), EnvSpec(Robotank-ramDeterministic-v4), EnvSpec(Robotank-ramNoFrameskip-v0), EnvSpec(Robotank-ramNoFrameskip-v4), EnvSpec(Seaquest-v0), EnvSpec(Seaquest-v4), EnvSpec(SeaquestDeterministic-v0), EnvSpec(SeaquestDeterministic-v4), EnvSpec(SeaquestNoFrameskip-v0), EnvSpec(SeaquestNoFrameskip-v4), EnvSpec(Seaquest-ram-v0), EnvSpec(Seaquest-ram-v4), EnvSpec(Seaquest-ramDeterministic-v0), EnvSpec(Seaquest-ramDeterministic-v4), EnvSpec(Seaquest-ramNoFrameskip-v0), EnvSpec(Seaquest-ramNoFrameskip-v4), EnvSpec(Skiing-v0), EnvSpec(Skiing-v4), EnvSpec(SkiingDeterministic-v0), EnvSpec(SkiingDeterministic-v4), EnvSpec(SkiingNoFrameskip-v0), EnvSpec(SkiingNoFrameskip-v4), EnvSpec(Skiing-ram-v0), EnvSpec(Skiing-ram-v4), EnvSpec(Skiing-ramDeterministic-v0), EnvSpec(Skiing-ramDeterministic-v4), EnvSpec(Skiing-ramNoFrameskip-v0), EnvSpec(Skiing-ramNoFrameskip-v4), EnvSpec(Solaris-v0), EnvSpec(Solaris-v4), EnvSpec(SolarisDeterministic-v0), EnvSpec(SolarisDeterministic-v4), EnvSpec(SolarisNoFrameskip-v0), EnvSpec(SolarisNoFrameskip-v4), EnvSpec(Solaris-ram-v0), EnvSpec(Solaris-ram-v4), EnvSpec(Solaris-ramDeterministic-v0), EnvSpec(Solaris-ramDeterministic-v4), EnvSpec(Solaris-ramNoFrameskip-v0), EnvSpec(Solaris-ramNoFrameskip-v4), EnvSpec(SpaceInvaders-v0), EnvSpec(SpaceInvaders-v4), EnvSpec(SpaceInvadersDeterministic-v0), EnvSpec(SpaceInvadersDeterministic-v4), EnvSpec(SpaceInvadersNoFrameskip-v0), EnvSpec(SpaceInvadersNoFrameskip-v4), EnvSpec(SpaceInvaders-ram-v0), EnvSpec(SpaceInvaders-ram-v4), EnvSpec(SpaceInvaders-ramDeterministic-v0), EnvSpec(SpaceInvaders-ramDeterministic-v4), EnvSpec(SpaceInvaders-ramNoFrameskip-v0), EnvSpec(SpaceInvaders-ramNoFrameskip-v4), EnvSpec(StarGunner-v0), EnvSpec(StarGunner-v4), EnvSpec(StarGunnerDeterministic-v0), EnvSpec(StarGunnerDeterministic-v4), EnvSpec(StarGunnerNoFrameskip-v0), EnvSpec(StarGunnerNoFrameskip-v4), EnvSpec(StarGunner-ram-v0), EnvSpec(StarGunner-ram-v4), EnvSpec(StarGunner-ramDeterministic-v0), EnvSpec(StarGunner-ramDeterministic-v4), EnvSpec(StarGunner-ramNoFrameskip-v0), EnvSpec(StarGunner-ramNoFrameskip-v4), EnvSpec(Tennis-v0), EnvSpec(Tennis-v4), EnvSpec(TennisDeterministic-v0), EnvSpec(TennisDeterministic-v4), EnvSpec(TennisNoFrameskip-v0), EnvSpec(TennisNoFrameskip-v4), EnvSpec(Tennis-ram-v0), EnvSpec(Tennis-ram-v4), EnvSpec(Tennis-ramDeterministic-v0), EnvSpec(Tennis-ramDeterministic-v4), EnvSpec(Tennis-ramNoFrameskip-v0), EnvSpec(Tennis-ramNoFrameskip-v4), EnvSpec(TimePilot-v0), EnvSpec(TimePilot-v4), EnvSpec(TimePilotDeterministic-v0), EnvSpec(TimePilotDeterministic-v4), EnvSpec(TimePilotNoFrameskip-v0), EnvSpec(TimePilotNoFrameskip-v4), EnvSpec(TimePilot-ram-v0), EnvSpec(TimePilot-ram-v4), EnvSpec(TimePilot-ramDeterministic-v0), EnvSpec(TimePilot-ramDeterministic-v4), EnvSpec(TimePilot-ramNoFrameskip-v0), EnvSpec(TimePilot-ramNoFrameskip-v4), EnvSpec(Tutankham-v0), EnvSpec(Tutankham-v4), EnvSpec(TutankhamDeterministic-v0), EnvSpec(TutankhamDeterministic-v4), EnvSpec(TutankhamNoFrameskip-v0), EnvSpec(TutankhamNoFrameskip-v4), EnvSpec(Tutankham-ram-v0), EnvSpec(Tutankham-ram-v4), EnvSpec(Tutankham-ramDeterministic-v0), EnvSpec(Tutankham-ramDeterministic-v4), EnvSpec(Tutankham-ramNoFrameskip-v0), EnvSpec(Tutankham-ramNoFrameskip-v4), EnvSpec(UpNDown-v0), EnvSpec(UpNDown-v4), EnvSpec(UpNDownDeterministic-v0), EnvSpec(UpNDownDeterministic-v4), EnvSpec(UpNDownNoFrameskip-v0), EnvSpec(UpNDownNoFrameskip-v4), EnvSpec(UpNDown-ram-v0), EnvSpec(UpNDown-ram-v4), EnvSpec(UpNDown-ramDeterministic-v0), EnvSpec(UpNDown-ramDeterministic-v4), EnvSpec(UpNDown-ramNoFrameskip-v0), EnvSpec(UpNDown-ramNoFrameskip-v4), EnvSpec(Venture-v0), EnvSpec(Venture-v4), EnvSpec(VentureDeterministic-v0), EnvSpec(VentureDeterministic-v4), EnvSpec(VentureNoFrameskip-v0), EnvSpec(VentureNoFrameskip-v4), EnvSpec(Venture-ram-v0), EnvSpec(Venture-ram-v4), EnvSpec(Venture-ramDeterministic-v0), EnvSpec(Venture-ramDeterministic-v4), EnvSpec(Venture-ramNoFrameskip-v0), EnvSpec(Venture-ramNoFrameskip-v4), EnvSpec(VideoPinball-v0), EnvSpec(VideoPinball-v4), EnvSpec(VideoPinballDeterministic-v0), EnvSpec(VideoPinballDeterministic-v4), EnvSpec(VideoPinballNoFrameskip-v0), EnvSpec(VideoPinballNoFrameskip-v4), EnvSpec(VideoPinball-ram-v0), EnvSpec(VideoPinball-ram-v4), EnvSpec(VideoPinball-ramDeterministic-v0), EnvSpec(VideoPinball-ramDeterministic-v4), EnvSpec(VideoPinball-ramNoFrameskip-v0), EnvSpec(VideoPinball-ramNoFrameskip-v4), EnvSpec(WizardOfWor-v0), EnvSpec(WizardOfWor-v4), EnvSpec(WizardOfWorDeterministic-v0), EnvSpec(WizardOfWorDeterministic-v4), EnvSpec(WizardOfWorNoFrameskip-v0), EnvSpec(WizardOfWorNoFrameskip-v4), EnvSpec(WizardOfWor-ram-v0), EnvSpec(WizardOfWor-ram-v4), EnvSpec(WizardOfWor-ramDeterministic-v0), EnvSpec(WizardOfWor-ramDeterministic-v4), EnvSpec(WizardOfWor-ramNoFrameskip-v0), EnvSpec(WizardOfWor-ramNoFrameskip-v4), EnvSpec(YarsRevenge-v0), EnvSpec(YarsRevenge-v4), EnvSpec(YarsRevengeDeterministic-v0), EnvSpec(YarsRevengeDeterministic-v4), EnvSpec(YarsRevengeNoFrameskip-v0), EnvSpec(YarsRevengeNoFrameskip-v4), EnvSpec(YarsRevenge-ram-v0), EnvSpec(YarsRevenge-ram-v4), EnvSpec(YarsRevenge-ramDeterministic-v0), EnvSpec(YarsRevenge-ramDeterministic-v4), EnvSpec(YarsRevenge-ramNoFrameskip-v0), EnvSpec(YarsRevenge-ramNoFrameskip-v4), EnvSpec(Zaxxon-v0), EnvSpec(Zaxxon-v4), EnvSpec(ZaxxonDeterministic-v0), EnvSpec(ZaxxonDeterministic-v4), EnvSpec(ZaxxonNoFrameskip-v0), EnvSpec(ZaxxonNoFrameskip-v4), EnvSpec(Zaxxon-ram-v0), EnvSpec(Zaxxon-ram-v4), EnvSpec(Zaxxon-ramDeterministic-v0), EnvSpec(Zaxxon-ramDeterministic-v4), EnvSpec(Zaxxon-ramNoFrameskip-v0), EnvSpec(Zaxxon-ramNoFrameskip-v4), EnvSpec(CubeCrash-v0), EnvSpec(CubeCrashSparse-v0), EnvSpec(CubeCrashScreenBecomesBlack-v0), EnvSpec(MemorizeDigits-v0)])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gym.envs.registry.all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Cart-Pole is a very simple environment composed of a cart that can move left or right, and pole placed vertically on top of it. The agent must move the cart left or right to keep the pole upright."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cart-pole](images/openai-gym.png \"Cart-Pole\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize the environment by calling is `reset()` method. This returns an observation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.seed(42)\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations vary depending on the environment. In this case it is a 1D NumPy array composed of 4 floats: they represent the cart's horizontal position, its velocity, the angle of the pole (0 = vertical), and the angular velocity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01258566, -0.00156614,  0.04207708, -0.00180545])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's view the environment:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cart-pole](images/cart-pole-image1.png \"Cart-Pole\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how to interact with an environment. Your agent will need to select an action from an \"action space\" (the set of possible actions). Let's see what this environment's action space looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yep, just two possible actions: accelerate towards the left or towards the right. Since the pole is leaning toward the right (`obs[2] > 0`), let's accelerate the cart toward the right:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.21266396,  1.71430093, -0.14444538, -1.7639987 ])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = 0  # accelerate right\n",
    "\n",
    "# The step() method executes the given action and returns four values\n",
    "obs, reward, done, info = env.step(action)\n",
    "\n",
    "# obs = This is the new observation\n",
    "\n",
    "# reward = In this environment, you get a reward of 1.0 at every step, no matter what you do,\n",
    "# so the goal is to keep the episode running as long as possible\n",
    "\n",
    "# done = This value will be True when the episode is over. This will happen when the pole\n",
    "# tilts too much, or goes off the screen, or after 200 steps (in this last case, you have\n",
    "# won). After that, the environment must be reset before it can be used again.\n",
    "\n",
    "# info = This environment-specific dictionary can provide some extra information that\n",
    "# you may find useful for debugging or for training.\n",
    "\n",
    "obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the cart is now moving toward the right (`obs[1] > 0`). The pole is still tilted toward the right (`obs[2] > 0`), but its angular velocity is now negative (`obs[3] < 0`), so it will likely be tilted toward the left after the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cart-pole](images/cart-pole-image2.png \"Cart-Pole\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like it's doing what we're telling it to do!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment also tells the agent how much reward it got during the last step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the game is over, the environment returns `done=True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, `info` is an environment-specific dictionary that can provide some extra information that you may find useful for debugging or for training. For example, in some games it may indicate how many lives the agent has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequence of steps between the moment the environment is reset until it is done is called an \"episode\". At the end of an episode (i.e., when `step()` returns `done=True`), you should reset the environment before you continue to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "if done:\n",
    "    obs = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now how can we make the poll remain upright? We will need to define a _policy_ for that. This is the strategy that the agent will use to select an action at each step. It can use all the past actions and observations to decide what to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Go Back to the Slides**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple hard-coded policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's hard code a simple strategy: if the pole is tilting to the left, then push the cart to the left, and _vice versa_. Let's see if that works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.seed(42)\n",
    "\n",
    "def basic_policy(obs):\n",
    "    angle = obs[2]\n",
    "    if angle < 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "totals = []\n",
    "for episode in range(500):\n",
    "    episode_rewards = 0\n",
    "    obs = env.reset()\n",
    "    for step in range(200):\n",
    "        action = basic_policy(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_rewards += reward\n",
    "        if done:\n",
    "            break\n",
    "    totals.append(episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[55.0,\n",
       " 25.0,\n",
       " 34.0,\n",
       " 31.0,\n",
       " 42.0,\n",
       " 39.0,\n",
       " 56.0,\n",
       " 49.0,\n",
       " 39.0,\n",
       " 39.0,\n",
       " 38.0,\n",
       " 37.0,\n",
       " 41.0,\n",
       " 38.0,\n",
       " 40.0,\n",
       " 49.0,\n",
       " 39.0,\n",
       " 24.0,\n",
       " 36.0,\n",
       " 35.0,\n",
       " 37.0,\n",
       " 42.0,\n",
       " 45.0,\n",
       " 34.0,\n",
       " 51.0,\n",
       " 40.0,\n",
       " 38.0,\n",
       " 39.0,\n",
       " 51.0,\n",
       " 31.0,\n",
       " 53.0,\n",
       " 60.0,\n",
       " 39.0,\n",
       " 42.0,\n",
       " 40.0,\n",
       " 52.0,\n",
       " 37.0,\n",
       " 41.0,\n",
       " 25.0,\n",
       " 37.0,\n",
       " 26.0,\n",
       " 25.0,\n",
       " 48.0,\n",
       " 46.0,\n",
       " 50.0,\n",
       " 47.0,\n",
       " 40.0,\n",
       " 34.0,\n",
       " 26.0,\n",
       " 41.0,\n",
       " 51.0,\n",
       " 46.0,\n",
       " 39.0,\n",
       " 52.0,\n",
       " 39.0,\n",
       " 45.0,\n",
       " 50.0,\n",
       " 38.0,\n",
       " 68.0,\n",
       " 46.0,\n",
       " 47.0,\n",
       " 35.0,\n",
       " 36.0,\n",
       " 35.0,\n",
       " 40.0,\n",
       " 52.0,\n",
       " 54.0,\n",
       " 39.0,\n",
       " 39.0,\n",
       " 36.0,\n",
       " 25.0,\n",
       " 45.0,\n",
       " 57.0,\n",
       " 49.0,\n",
       " 47.0,\n",
       " 39.0,\n",
       " 37.0,\n",
       " 49.0,\n",
       " 51.0,\n",
       " 39.0,\n",
       " 31.0,\n",
       " 25.0,\n",
       " 52.0,\n",
       " 47.0,\n",
       " 39.0,\n",
       " 34.0,\n",
       " 35.0,\n",
       " 46.0,\n",
       " 34.0,\n",
       " 49.0,\n",
       " 47.0,\n",
       " 54.0,\n",
       " 47.0,\n",
       " 45.0,\n",
       " 45.0,\n",
       " 56.0,\n",
       " 25.0,\n",
       " 41.0,\n",
       " 48.0,\n",
       " 39.0,\n",
       " 39.0,\n",
       " 51.0,\n",
       " 33.0,\n",
       " 39.0,\n",
       " 26.0,\n",
       " 40.0,\n",
       " 25.0,\n",
       " 39.0,\n",
       " 49.0,\n",
       " 36.0,\n",
       " 34.0,\n",
       " 38.0,\n",
       " 36.0,\n",
       " 55.0,\n",
       " 31.0,\n",
       " 26.0,\n",
       " 52.0,\n",
       " 61.0,\n",
       " 40.0,\n",
       " 37.0,\n",
       " 25.0,\n",
       " 57.0,\n",
       " 56.0,\n",
       " 45.0,\n",
       " 36.0,\n",
       " 64.0,\n",
       " 51.0,\n",
       " 45.0,\n",
       " 46.0,\n",
       " 41.0,\n",
       " 48.0,\n",
       " 52.0,\n",
       " 41.0,\n",
       " 44.0,\n",
       " 32.0,\n",
       " 35.0,\n",
       " 49.0,\n",
       " 40.0,\n",
       " 39.0,\n",
       " 25.0,\n",
       " 38.0,\n",
       " 45.0,\n",
       " 35.0,\n",
       " 39.0,\n",
       " 38.0,\n",
       " 44.0,\n",
       " 52.0,\n",
       " 38.0,\n",
       " 54.0,\n",
       " 52.0,\n",
       " 48.0,\n",
       " 60.0,\n",
       " 25.0,\n",
       " 48.0,\n",
       " 39.0,\n",
       " 42.0,\n",
       " 41.0,\n",
       " 36.0,\n",
       " 41.0,\n",
       " 46.0,\n",
       " 46.0,\n",
       " 25.0,\n",
       " 50.0,\n",
       " 52.0,\n",
       " 40.0,\n",
       " 26.0,\n",
       " 43.0,\n",
       " 41.0,\n",
       " 34.0,\n",
       " 51.0,\n",
       " 47.0,\n",
       " 38.0,\n",
       " 53.0,\n",
       " 35.0,\n",
       " 39.0,\n",
       " 42.0,\n",
       " 38.0,\n",
       " 53.0,\n",
       " 46.0,\n",
       " 47.0,\n",
       " 67.0,\n",
       " 49.0,\n",
       " 51.0,\n",
       " 25.0,\n",
       " 36.0,\n",
       " 40.0,\n",
       " 41.0,\n",
       " 37.0,\n",
       " 39.0,\n",
       " 38.0,\n",
       " 45.0,\n",
       " 45.0,\n",
       " 49.0,\n",
       " 51.0,\n",
       " 32.0,\n",
       " 36.0,\n",
       " 56.0,\n",
       " 39.0,\n",
       " 39.0,\n",
       " 41.0,\n",
       " 38.0,\n",
       " 40.0,\n",
       " 45.0,\n",
       " 40.0,\n",
       " 39.0,\n",
       " 51.0,\n",
       " 34.0,\n",
       " 26.0,\n",
       " 42.0,\n",
       " 57.0,\n",
       " 48.0,\n",
       " 37.0,\n",
       " 51.0,\n",
       " 37.0,\n",
       " 41.0,\n",
       " 40.0,\n",
       " 48.0,\n",
       " 39.0,\n",
       " 68.0,\n",
       " 47.0,\n",
       " 42.0,\n",
       " 53.0,\n",
       " 39.0,\n",
       " 53.0,\n",
       " 51.0,\n",
       " 46.0,\n",
       " 34.0,\n",
       " 55.0,\n",
       " 39.0,\n",
       " 34.0,\n",
       " 39.0,\n",
       " 56.0,\n",
       " 46.0,\n",
       " 38.0,\n",
       " 27.0,\n",
       " 37.0,\n",
       " 42.0,\n",
       " 34.0,\n",
       " 25.0,\n",
       " 38.0,\n",
       " 42.0,\n",
       " 52.0,\n",
       " 39.0,\n",
       " 56.0,\n",
       " 51.0,\n",
       " 45.0,\n",
       " 41.0,\n",
       " 53.0,\n",
       " 34.0,\n",
       " 38.0,\n",
       " 42.0,\n",
       " 41.0,\n",
       " 59.0,\n",
       " 45.0,\n",
       " 37.0,\n",
       " 26.0,\n",
       " 43.0,\n",
       " 26.0,\n",
       " 47.0,\n",
       " 39.0,\n",
       " 39.0,\n",
       " 48.0,\n",
       " 37.0,\n",
       " 39.0,\n",
       " 59.0,\n",
       " 38.0,\n",
       " 37.0,\n",
       " 35.0,\n",
       " 51.0,\n",
       " 53.0,\n",
       " 46.0,\n",
       " 42.0,\n",
       " 37.0,\n",
       " 35.0,\n",
       " 40.0,\n",
       " 51.0,\n",
       " 42.0,\n",
       " 40.0,\n",
       " 46.0,\n",
       " 45.0,\n",
       " 31.0,\n",
       " 42.0,\n",
       " 51.0,\n",
       " 35.0,\n",
       " 53.0,\n",
       " 46.0,\n",
       " 46.0,\n",
       " 47.0,\n",
       " 34.0,\n",
       " 39.0,\n",
       " 37.0,\n",
       " 49.0,\n",
       " 26.0,\n",
       " 39.0,\n",
       " 35.0,\n",
       " 47.0,\n",
       " 48.0,\n",
       " 47.0,\n",
       " 40.0,\n",
       " 45.0,\n",
       " 25.0,\n",
       " 61.0,\n",
       " 38.0,\n",
       " 38.0,\n",
       " 62.0,\n",
       " 35.0,\n",
       " 41.0,\n",
       " 35.0,\n",
       " 52.0,\n",
       " 39.0,\n",
       " 51.0,\n",
       " 25.0,\n",
       " 52.0,\n",
       " 43.0,\n",
       " 26.0,\n",
       " 52.0,\n",
       " 59.0,\n",
       " 32.0,\n",
       " 41.0,\n",
       " 37.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 35.0,\n",
       " 39.0,\n",
       " 40.0,\n",
       " 47.0,\n",
       " 39.0,\n",
       " 40.0,\n",
       " 42.0,\n",
       " 55.0,\n",
       " 41.0,\n",
       " 34.0,\n",
       " 51.0,\n",
       " 37.0,\n",
       " 25.0,\n",
       " 36.0,\n",
       " 35.0,\n",
       " 40.0,\n",
       " 37.0,\n",
       " 38.0,\n",
       " 37.0,\n",
       " 25.0,\n",
       " 39.0,\n",
       " 53.0,\n",
       " 48.0,\n",
       " 56.0,\n",
       " 48.0,\n",
       " 34.0,\n",
       " 41.0,\n",
       " 41.0,\n",
       " 35.0,\n",
       " 50.0,\n",
       " 43.0,\n",
       " 53.0,\n",
       " 25.0,\n",
       " 31.0,\n",
       " 37.0,\n",
       " 35.0,\n",
       " 40.0,\n",
       " 64.0,\n",
       " 41.0,\n",
       " 36.0,\n",
       " 36.0,\n",
       " 47.0,\n",
       " 37.0,\n",
       " 42.0,\n",
       " 36.0,\n",
       " 52.0,\n",
       " 25.0,\n",
       " 35.0,\n",
       " 37.0,\n",
       " 39.0,\n",
       " 25.0,\n",
       " 34.0,\n",
       " 57.0,\n",
       " 39.0,\n",
       " 42.0,\n",
       " 39.0,\n",
       " 40.0,\n",
       " 68.0,\n",
       " 39.0,\n",
       " 26.0,\n",
       " 58.0,\n",
       " 47.0,\n",
       " 31.0,\n",
       " 31.0,\n",
       " 24.0,\n",
       " 35.0,\n",
       " 49.0,\n",
       " 41.0,\n",
       " 53.0,\n",
       " 50.0,\n",
       " 55.0,\n",
       " 41.0,\n",
       " 52.0,\n",
       " 38.0,\n",
       " 48.0,\n",
       " 55.0,\n",
       " 38.0,\n",
       " 25.0,\n",
       " 56.0,\n",
       " 55.0,\n",
       " 38.0,\n",
       " 47.0,\n",
       " 41.0,\n",
       " 53.0,\n",
       " 25.0,\n",
       " 42.0,\n",
       " 47.0,\n",
       " 56.0,\n",
       " 39.0,\n",
       " 54.0,\n",
       " 27.0,\n",
       " 51.0,\n",
       " 51.0,\n",
       " 35.0,\n",
       " 42.0,\n",
       " 45.0,\n",
       " 48.0,\n",
       " 45.0,\n",
       " 35.0,\n",
       " 47.0,\n",
       " 35.0,\n",
       " 43.0,\n",
       " 34.0,\n",
       " 47.0,\n",
       " 36.0,\n",
       " 25.0,\n",
       " 45.0,\n",
       " 50.0,\n",
       " 41.0,\n",
       " 43.0,\n",
       " 38.0,\n",
       " 48.0,\n",
       " 60.0,\n",
       " 46.0,\n",
       " 45.0,\n",
       " 35.0,\n",
       " 36.0,\n",
       " 38.0,\n",
       " 45.0,\n",
       " 46.0,\n",
       " 50.0,\n",
       " 26.0,\n",
       " 25.0,\n",
       " 25.0,\n",
       " 41.0,\n",
       " 62.0,\n",
       " 51.0,\n",
       " 25.0,\n",
       " 36.0,\n",
       " 32.0,\n",
       " 50.0,\n",
       " 38.0,\n",
       " 39.0,\n",
       " 45.0,\n",
       " 40.0,\n",
       " 41.0,\n",
       " 41.0,\n",
       " 26.0,\n",
       " 43.0,\n",
       " 35.0,\n",
       " 43.0,\n",
       " 38.0,\n",
       " 41.0,\n",
       " 40.0,\n",
       " 39.0,\n",
       " 38.0,\n",
       " 54.0,\n",
       " 32.0,\n",
       " 52.0,\n",
       " 32.0,\n",
       " 36.0,\n",
       " 42.0,\n",
       " 25.0,\n",
       " 25.0,\n",
       " 36.0,\n",
       " 53.0,\n",
       " 35.0,\n",
       " 45.0,\n",
       " 31.0,\n",
       " 25.0,\n",
       " 51.0,\n",
       " 36.0,\n",
       " 43.0,\n",
       " 48.0,\n",
       " 40.0,\n",
       " 34.0,\n",
       " 39.0,\n",
       " 39.0,\n",
       " 55.0,\n",
       " 37.0,\n",
       " 35.0,\n",
       " 46.0,\n",
       " 35.0,\n",
       " 38.0,\n",
       " 52.0,\n",
       " 56.0,\n",
       " 49.0,\n",
       " 36.0]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s look at the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41.718, 8.858356280936096, 24.0, 68.0)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(totals), np.std(totals), np.min(totals), np.max(totals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, as expected, this strategy is a bit too basic: the best it did was to keep the poll up for only 68 steps. This environment is considered solved when the agent keeps the poll up for 200 steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize one episode and see the animation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<video controls src=\"videos/cart-pole-video1.mp4\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly the system is unstable and after just a few wobbles, the pole ends up too tilted: game over. We will need to be smarter than that!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a neural network that will take observations as inputs, and output the action to take for each observation. To choose an action, the network will estimate a probability for each action, then we will select an action randomly according to the estimated probabilities. In the case of the Cart-Pole environment, there are just two possible actions (left or right), so we only need one output neuron: it will output the probability `p` of the action 0 (left), and of course the probability of action 1 (right) will be `1 - p`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "n_inputs = 4 # == env.observation_space.shape[0]\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(5, activation=\"elu\", input_shape=[n_inputs]),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this particular environment, the past actions and observations can safely be ignored, since each observation contains the environment's full state. If there were some hidden state then you may need to consider past actions and observations in order to try to infer the hidden state of the environment. For example, if the environment only revealed the position of the cart but not its velocity, you would have to consider not only the current observation but also the previous observation in order to estimate the current velocity. Another example is if the observations are noisy: you may want to use the past few observations to estimate the most likely current state. Our problem is thus as simple as can be: the current observation is noise-free and contains the environment's full state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may wonder why we plan to pick a random action based on the probability given by the policy network, rather than just picking the action with the highest probability. This approach lets the agent find the right balance between _exploring_ new actions and _exploiting_ the actions that are known to work well. Here's an analogy: suppose you go to a restaurant for the first time, and all the dishes look equally appealing so you randomly pick one. If it turns out to be good, you can increase the probability to order it next time, but you shouldn't increase that probability to 100%, or else you will never try out the other dishes, some of which may be even better than the one you tried."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a small function that will run the model to play one episode, and return the frames so we can display an animation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_policy_net(model, n_max_steps=200, seed=42):\n",
    "    frames = []\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    env._max_episode_steps = n_max_steps\n",
    "    env.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    obs = env.reset()\n",
    "    for step in range(n_max_steps):\n",
    "        frames.append(1) #env.render(mode=\"rgb_array\"))\n",
    "        left_proba = model.predict(obs.reshape(1, -1))\n",
    "        action = int(np.random.rand() > left_proba) #if left_proba = 0.7, action = 1 30%, action = 0 70%\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "    env.close()\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at how well this randomly initialized policy network performs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<video controls src=\"videos/cart-pole-video2.mp4\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeah... pretty bad. The neural network will have to learn to do better. First let's see if it is capable of learning the basic policy we used earlier: go left if the pole is tilting left, and go right if it is tilting right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make the same net play in 50 different environments in parallel (this will give us a diverse training batch at each step), and train for 5000 iterations. We also reset environments when they are done. We train the model using a custom training loop so we can easily use the predictions at each training step to advance the environments.\n",
    "\n",
    "**Please note:** We have set the number of iterations to 50 in order to reduce the execution time during lectures. Please feel free to uncomment the original code and run it in the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4999, Loss: 0.019"
     ]
    }
   ],
   "source": [
    "n_environments = 50\n",
    "# n_iterations = 5000\n",
    "n_iterations = 5000\n",
    "\n",
    "envs = [gym.make(\"CartPole-v1\") for _ in range(n_environments)]\n",
    "for index, env in enumerate(envs):\n",
    "    env.seed(index)\n",
    "np.random.seed(42)\n",
    "observations = [env.reset() for env in envs]\n",
    "optimizer = keras.optimizers.RMSprop()\n",
    "loss_fn = keras.losses.binary_crossentropy\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    # if angle < 0, we want proba(left) = 1., or else proba(left) = 0.\n",
    "    target_probas = np.array([([1.] if obs[2] < 0 else [0.])\n",
    "                              for obs in observations])\n",
    "    #model.fit(np.array(observations), target_probas, optimizer=optimizer, loss=loss_fn)\n",
    "    with tf.GradientTape() as tape:\n",
    "        left_probas = model(np.array(observations))\n",
    "        loss = tf.reduce_mean(loss_fn(target_probas, left_probas))\n",
    "    print(\"\\rIteration: {}, Loss: {:.3f}\".format(iteration, loss.numpy()), end=\"\")\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    # end model.fit\n",
    "    \n",
    "    actions = (np.random.rand(n_environments, 1) > left_probas.numpy()).astype(np.int32)\n",
    "    for env_index, env in enumerate(envs):\n",
    "        obs, reward, done, info = env.step(actions[env_index][0])\n",
    "        observations[env_index] = obs if not done else env.reset()\n",
    "\n",
    "for env in envs:\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = render_policy_net(model, seed=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def nn_policy(obs):\n",
    "    prob = model.predict(np.array(obs).reshape(1,-1))\n",
    "    if prob > 0.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "totals = []\n",
    "for episode in range(50):\n",
    "    episode_rewards = 0\n",
    "    obs = env.reset()\n",
    "    for step in range(200):\n",
    "        action = nn_policy(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_rewards += reward\n",
    "        if done:\n",
    "            break\n",
    "    totals.append(episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8.9, 0.5, 8.0, 10.0)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(totals), np.std(totals), np.min(totals), np.max(totals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<video controls src=\"videos/cart-pole-video3.mp4\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like it learned the policy correctly. Now let's see if it can learn a better policy on its own. One that does not wobble as much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Go Back to the Slides**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train this neural network we will need to define the target probabilities `y`. If an action is good we should increase its probability, and conversely if it is bad we should reduce it. But how do we know whether an action is good or bad? The problem is that most actions have delayed effects, so when you win or lose points in an episode, it is not clear which actions contributed to this result: was it just the last action? Or the last 10? Or just one action 50 steps earlier? This is called the _credit assignment problem_.\n",
    "\n",
    "The _Policy Gradients_ algorithm tackles this problem by first playing multiple episodes, then making the actions in good episodes slightly more likely, while actions in bad episodes are made slightly less likely. First we play, then we go back and think about what we did."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by creating a function to play a single step using the model. We will also pretend for now that whatever action it takes is the right one, so we can compute the loss and its gradients (we will just save these gradients for now, and modify them later depending on how good or bad the action turned out to be):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_step(env, obs, model, loss_fn):\n",
    "    with tf.GradientTape() as tape:\n",
    "        left_proba = model(obs[np.newaxis])\n",
    "        action = (tf.random.uniform([1, 1]) > left_proba)\n",
    "        y_target = tf.constant([[1.]]) - tf.cast(action, tf.float32)\n",
    "        loss = tf.reduce_mean(loss_fn(y_target, left_proba))\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    obs, reward, done, info = env.step(int(action[0, 0].numpy()))\n",
    "    return obs, reward, done, grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `left_proba` is high, then `action` will most likely be `False` (since a random number uniformally sampled between 0 and 1 will probably not be greater than `left_proba`). And `False` means 0 when you cast it to a number, so `y_target` would be equal to 1 - 0 = 1. In other words, we set the target to 1, meaning we pretend that the probability of going left should have been 100% (so we took the right action)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create another function that will rely on the `play_one_step()` function to play multiple episodes, returning all the rewards and gradients, for each episode and each step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_multiple_episodes(env, n_episodes, n_max_steps, model, loss_fn):\n",
    "    all_rewards = []\n",
    "    all_grads = []\n",
    "    for episode in range(n_episodes):\n",
    "        current_rewards = []\n",
    "        current_grads = []\n",
    "        obs = env.reset()\n",
    "        for step in range(n_max_steps):\n",
    "            obs, reward, done, grads = play_one_step(env, obs, model, loss_fn)\n",
    "            current_rewards.append(reward)\n",
    "            current_grads.append(grads)\n",
    "            if done:\n",
    "                break\n",
    "        all_rewards.append(current_rewards)\n",
    "        all_grads.append(current_grads)\n",
    "    return all_rewards, all_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Policy Gradients algorithm uses the model to play the episode several times (e.g., 10 times), then it goes back and looks at all the rewards, discounts them and normalizes them. So let's create couple functions for that: the first will compute discounted rewards; the second will normalize the discounted rewards across many episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, discount_rate):\n",
    "    discounted = np.array(rewards)\n",
    "    for step in range(len(rewards) - 2, -1, -1):\n",
    "        discounted[step] += discounted[step + 1] * discount_rate\n",
    "    return discounted\n",
    "\n",
    "def discount_and_normalize_rewards(all_rewards, discount_rate):\n",
    "    all_discounted_rewards = [discount_rewards(rewards, discount_rate)\n",
    "                              for rewards in all_rewards]\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    reward_std = flat_rewards.std()\n",
    "    return [(discounted_rewards - reward_mean) / reward_std\n",
    "            for discounted_rewards in all_discounted_rewards]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say there were 3 actions, and after each action there was a reward: first 10, then 0, then -50. If we use a discount factor of 80%, then the 3rd action will get -50 (full credit for the last reward), but the 2nd action will only get -40 (80% credit for the last reward), and the 1st action will get 80% of -40 (-32) plus full credit for the first reward (+10), which leads to a discounted reward of -22:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -44,  -56,  -70, -100])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discount_rewards([0, 0, 10, -100], discount_rate=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To normalize all discounted rewards across all episodes, we compute the mean and standard deviation of all the discounted rewards, and we subtract the mean from each discounted reward, and divide by the standard deviation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.28435071, -0.86597718, -1.18910299]),\n",
       " array([1.26665318, 1.0727777 ])]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discount_and_normalize_rewards([[10, 0, -50], [10, 20]], discount_rate=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please note:** We have set the number of iterations to 10, episodes per update to 5, and max steps to 50 in order to reduce the execution time during lectures. Please feel free to uncomment the original code and run it in the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_iterations = 150\n",
    "n_iterations = 100\n",
    "# n_episodes_per_update = 10\n",
    "n_episodes_per_update = 5\n",
    "# n_max_steps = 200\n",
    "n_max_steps = 2000\n",
    "discount_rate = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(lr=0.01)\n",
    "loss_fn = keras.losses.binary_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(6, activation=\"elu\", input_shape=[4]),\n",
    "    keras.layers.Dense(6, activation=\"elu\", input_shape=[4]),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each training iteration, this loop calls the `play_multiple_episodes()` function, which plays the game 10 times and returns all the rewards and gradients for every episode and step.\n",
    "\n",
    "Then we call the `discount_and_normalize_rewards()` to compute each action’s normalized advantage (which in the code we call the `final_reward`). This provides a measure of how good or bad each action actually was, in hindsight.\n",
    "\n",
    "Next, we go through each trainable variable, and for each of them we compute the weighted mean of the gradients for that variable over all episodes and all steps, weighted by the `final_reward`.\n",
    "\n",
    "Finally, we apply these mean gradients using the optimizer: the model’s trainable variables will be tweaked, and hopefully the policy will be a bit better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Iteration: 0, mean rewards: 2000.0"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-30bfc8aff2a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     all_rewards, all_grads = play_multiple_episodes(\n\u001b[0;32m----> 7\u001b[0;31m         env, n_episodes_per_update, n_max_steps, model, loss_fn)\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtotal_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m                     \u001b[0;31m# Not shown in the book\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     print(\"\\rIteration: {}, mean rewards: {:.1f}\".format(          # Not shown\n",
      "\u001b[0;32m<ipython-input-8-e190097d0502>\u001b[0m in \u001b[0;36mplay_multiple_episodes\u001b[0;34m(env, n_episodes, n_max_steps, model, loss_fn)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_max_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplay_one_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mcurrent_rewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mcurrent_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-ad0d62d46cfd>\u001b[0m in \u001b[0;36mplay_one_step\u001b[0;34m(env, obs, model, loss_fn)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplay_one_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mleft_proba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mleft_proba\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0my_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda/envs/tensorflow2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    668\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_convert_non_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m       \u001b[0minput_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda/envs/tensorflow2/lib/python3.6/site-packages/tensorflow_core/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda/envs/tensorflow2/lib/python3.6/site-packages/tensorflow_core/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda/envs/tensorflow2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_convert_non_tensor\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    662\u001b[0m     \u001b[0;31m# Accept NumPy and scalar inputs by converting to Tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m       \u001b[0;32mdef\u001b[0m \u001b[0m_convert_non_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    665\u001b[0m         \u001b[0;31m# Don't call `ops.convert_to_tensor` on all `inputs` because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;31m# `SparseTensors` can't be converted to `Tensor`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "env._max_episode_steps = 5000\n",
    "env.seed(42);\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    all_rewards, all_grads = play_multiple_episodes(\n",
    "        env, n_episodes_per_update, n_max_steps, model, loss_fn)\n",
    "    total_rewards = sum(map(sum, all_rewards))                     # Not shown in the book\n",
    "    print(\"\\rIteration: {}, mean rewards: {:.1f}\".format(          # Not shown\n",
    "        iteration, total_rewards / n_episodes_per_update), end=\"\") # Not shown\n",
    "    all_final_rewards = discount_and_normalize_rewards(all_rewards,\n",
    "                                                       discount_rate)\n",
    "    all_mean_grads = []\n",
    "    for var_index in range(len(model.trainable_variables)):\n",
    "        mean_grads = tf.reduce_mean(\n",
    "            [final_reward * all_grads[episode_index][step][var_index]\n",
    "             for episode_index, final_rewards in enumerate(all_final_rewards)\n",
    "                 for step, final_reward in enumerate(final_rewards)], axis=0)\n",
    "        all_mean_grads.append(mean_grads)\n",
    "    optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    frames = render_policy_net(model, n_max_steps=2000, seed=i)\n",
    "    print(len(frames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<video controls src=\"videos/cart-pole-video4.mp4\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simple policy gradients algorithm we just trained solved the CartPole task, but it would not scale well to larger and more complex tasks. Indeed, it is highly sample inefficient, meaning it needs to explore the game for a very long time before it can\n",
    "make significant progress. This is due to the fact that it must run multiple episodes to estimate the advantage of each action, as we have seen. However, it is the foundation of more powerful algorithms, such as Actor-Critic algorithms (which we will discuss briefly at the end of this chapter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Go Back to the Slides**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States: 0 0 3 \n",
      "States: 0 1 2 1 2 1 2 1 2 1 3 \n",
      "States: 0 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 3 \n",
      "States: 0 3 \n",
      "States: 0 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 3 \n",
      "States: 0 1 3 \n",
      "States: 0 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 ...\n",
      "States: 0 0 3 \n",
      "States: 0 0 0 1 2 1 2 1 3 \n",
      "States: 0 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 3 \n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "transition_probabilities = [ # shape=[s, s']\n",
    "        [0.7, 0.2, 0.0, 0.1],  # from s0 to s0, s1, s2, s3\n",
    "        [0.0, 0.0, 0.9, 0.1],  # from s1 to ...\n",
    "        [0.0, 1.0, 0.0, 0.0],  # from s2 to ...\n",
    "        [0.0, 0.0, 0.0, 1.0]]  # from s3 to ...\n",
    "\n",
    "n_max_steps = 50\n",
    "\n",
    "def print_sequence():\n",
    "    current_state = 0\n",
    "    print(\"States:\", end=\" \")\n",
    "    for step in range(n_max_steps):\n",
    "        print(current_state, end=\" \")\n",
    "        if current_state == 3:\n",
    "            break\n",
    "        current_state = np.random.choice(range(4), p=transition_probabilities[current_state])\n",
    "    else:\n",
    "        print(\"...\", end=\"\")\n",
    "    print()\n",
    "\n",
    "for _ in range(10):\n",
    "    print_sequence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define some transition probabilities, rewards and possible actions. For example, in state s0, if action a0 is chosen then with proba 0.7 we will go to state s0 with reward +10, with probability 0.3 we will go to state s1 with no reward, and with never go to state s2 (so the transition probabilities are `[0.7, 0.3, 0.0]`, and the rewards are `[+10, 0, 0]`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_probabilities = [ # shape=[s, a, s']\n",
    "        [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]],\n",
    "        [[0.0, 1.0, 0.0], None, [0.0, 0.0, 1.0]],\n",
    "        [None, [0.8, 0.1, 0.1], None]]\n",
    "rewards = [ # shape=[s, a, s']\n",
    "        [[+10, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
    "        [[0, 0, 0], [0, 0, 0], [0, 0, -50]],\n",
    "        [[0, 0, 0], [+40, 0, 0], [0, 0, 0]]]\n",
    "possible_actions = [[0, 1, 2], [0, 2], [1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will initialize all the Q-Values to 0 (except for the the impossible actions, for which we set the Q-Values to –∞):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_values = np.full((3, 3), -np.inf) # -np.inf for impossible actions\n",
    "for state, actions in enumerate(possible_actions):\n",
    "    Q_values[state, actions] = 0.0  # for all possible actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the Q-Value Iteration algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.90  # the discount factor\n",
    "\n",
    "history1 = [] # Not shown in the book (for the figure below)\n",
    "for iteration in range(50):\n",
    "    Q_prev = Q_values.copy()\n",
    "    history1.append(Q_prev) # Not shown\n",
    "    for s in range(3):\n",
    "        for a in possible_actions[s]:\n",
    "            Q_values[s, a] = np.sum([\n",
    "                    transition_probabilities[s][a][sp]\n",
    "                    * (rewards[s][a][sp] + gamma * np.max(Q_prev[sp]))\n",
    "                for sp in range(3)])\n",
    "\n",
    "history1 = np.array(history1) # Not shown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That’s it! The resulting Q-Values look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[18.91891892, 17.02702702, 13.62162162],\n",
       "       [ 0.        ,        -inf, -4.87971488],\n",
       "       [       -inf, 50.13365013,        -inf]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each state, let’s look at the action that has the highest Q-Value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(Q_values, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal policy for this MDP, when using a discount factor of 0.90, is to choose action a0 when in state s0, and choose action a0 when in state s1, and finally choose action a1 (the only possible action) when in state s2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try again with a discount factor of 0.95:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_values = np.full((3, 3), -np.inf) # -np.inf for impossible actions\n",
    "for state, actions in enumerate(possible_actions):\n",
    "    Q_values[state, actions] = 0.0  # for all possible actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.95  # the discount factor\n",
    "\n",
    "for iteration in range(50):\n",
    "    Q_prev = Q_values.copy()\n",
    "    for s in range(3):\n",
    "        for a in possible_actions[s]:\n",
    "            Q_values[s, a] = np.sum([\n",
    "                    transition_probabilities[s][a][sp]\n",
    "                    * (rewards[s][a][sp] + gamma * np.max(Q_prev[sp]))\n",
    "                for sp in range(3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[21.73304188, 20.63807938, 16.70138772],\n",
       "       [ 0.95462106,        -inf,  1.01361207],\n",
       "       [       -inf, 53.70728682,        -inf]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 1])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(Q_values, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the policy has changed! In state s1, we now prefer to go through the fire (choose action a2). This is because the discount factor is larger so the agent values the future more, and it is therefore ready to pay an immediate penalty in order to get more future rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Go Back to Slides**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Learning works by watching an agent play (e.g., randomly) and gradually improving its estimates of the Q-Values. Once it has accurate Q-Value estimates (or close enough), then the optimal policy consists in choosing the action that has the highest Q-Value (i.e., the greedy policy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to simulate an agent moving around in the environment, so let's define a function to perform some action and get the new state and a reward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(state, action):\n",
    "    probas = transition_probabilities[state][action]\n",
    "    next_state = np.random.choice([0, 1, 2], p=probas)\n",
    "    reward = rewards[state][action][next_state]\n",
    "    return next_state, reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need an exploration policy, which can be any policy, as long as it visits every possible state many times. We will just use a random policy, since the state space is very small:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exploration_policy(state):\n",
    "    return np.random.choice(possible_actions[state])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's initialize the Q-Values like earlier, and run the Q-Learning algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "Q_values = np.full((3, 3), -np.inf)\n",
    "for state, actions in enumerate(possible_actions):\n",
    "    Q_values[state][actions] = 0\n",
    "\n",
    "alpha0 = 0.05 # initial learning rate\n",
    "decay = 0.005 # learning rate decay\n",
    "gamma = 0.90 # discount factor\n",
    "state = 0 # initial state\n",
    "history2 = [] # Not shown in the book\n",
    "\n",
    "for iteration in range(10000):\n",
    "    history2.append(Q_values.copy()) # Not shown\n",
    "    action = exploration_policy(state)\n",
    "    next_state, reward = step(state, action)\n",
    "    next_value = np.max(Q_values[next_state]) # greedy policy at the next step\n",
    "    alpha = alpha0 / (1 + iteration * decay)\n",
    "    Q_values[state, action] *= 1 - alpha\n",
    "    Q_values[state, action] += alpha * (reward + gamma * next_value)\n",
    "    state = next_state\n",
    "\n",
    "history2 = np.array(history2) # Not shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[18.77621289, 17.2238872 , 13.74543343],\n",
       "       [ 0.        ,        -inf, -8.00485647],\n",
       "       [       -inf, 49.40208921,        -inf]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(Q_values, axis=1) # optimal action for each state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "true_Q_value = history1[-1, 0, 0]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4), sharey=True)\n",
    "axes[0].set_ylabel(\"Q-Value$(s_0, a_0)$\", fontsize=14)\n",
    "axes[0].set_title(\"Q-Value Iteration\", fontsize=14)\n",
    "axes[1].set_title(\"Q-Learning\", fontsize=14)\n",
    "for ax, width, history in zip(axes, (50, 10000), (history1, history2)):\n",
    "    ax.plot([0, width], [true_Q_value, true_Q_value], \"k--\")\n",
    "    ax.plot(np.arange(width), history[:, 0, 0], \"b-\", linewidth=2)\n",
    "    ax.set_xlabel(\"Iterations\", fontsize=14)\n",
    "    ax.axis([0, width, 0, 24])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Q-Value Iteration algorithm (left) converges very quickly, in fewer than 20 iterations, while the Q-Learning algorithm (right) takes about 8,000 iterations to converge. Obviously, not knowing the transition probabilities or the rewards makes finding the optimal policy significantly harder!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Go Back to the Slides**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the DQN. Given a state, it will estimate, for each possible action, the sum of discounted future rewards it can expect after it plays that action (but before it sees its outcome):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "input_shape = [4] # == env.observation_space.shape\n",
    "n_outputs = 2 # == env.action_space.n\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(32, activation=\"elu\", input_shape=input_shape),\n",
    "    keras.layers.Dense(32, activation=\"elu\"),\n",
    "    keras.layers.Dense(n_outputs)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To select an action using this DQN, we just pick the action with the largest predicted Q-value. However, to ensure that the agent explores the environment, we choose a random action with probability `epsilon`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state, epsilon=0):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(2)\n",
    "    else:\n",
    "        Q_values = model.predict(state[np.newaxis])\n",
    "        return np.argmax(Q_values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also need a replay memory. It will contain the agent's experiences, in the form of tuples: `(obs, action, reward, next_obs, done)`. We can use the `deque` class for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "replay_memory = deque(maxlen=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's create a function to sample experiences from the replay memory. It will return 5 NumPy arrays: `[obs, actions, rewards, next_obs, dones]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_experiences(batch_size):\n",
    "    indices = np.random.randint(len(replay_memory), size=batch_size)\n",
    "    batch = [replay_memory[index] for index in indices]\n",
    "    states, actions, rewards, next_states, dones = [\n",
    "        np.array([experience[field_index] for experience in batch])\n",
    "        for field_index in range(5)]\n",
    "    return states, actions, rewards, next_states, dones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a function that will use the DQN to play one step, and record its experience in the replay memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_step(env, state, epsilon):\n",
    "    action = epsilon_greedy_policy(state, epsilon)\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    replay_memory.append((state, action, reward, next_state, done))\n",
    "    return next_state, reward, done, info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's create a function that will sample some experiences from the replay memory and perform a training step:\n",
    "\n",
    "**Note**: the first 3 releases of the 2nd edition were missing the `reshape()` operation which converts `target_Q_values` to a column vector (this is required by the `loss_fn()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "discount_rate = 0.95\n",
    "optimizer = keras.optimizers.Adam(lr=1e-3)\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "\n",
    "def training_step(batch_size):\n",
    "    experiences = sample_experiences(batch_size)\n",
    "    states, actions, rewards, next_states, dones = experiences\n",
    "    next_Q_values = model.predict(next_states)\n",
    "    max_next_Q_values = np.max(next_Q_values, axis=1)\n",
    "    target_Q_values = (rewards +\n",
    "                       (1 - dones) * discount_rate * max_next_Q_values)\n",
    "    target_Q_values = target_Q_values.reshape(-1, 1)\n",
    "    mask = tf.one_hot(actions, n_outputs)\n",
    "    with tf.GradientTape() as tape:\n",
    "        all_Q_values = model(states)\n",
    "        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
    "        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, let's train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "rewards = [] \n",
    "best_score = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please note:** We have set the number of episode range to 100, and step range to 50 in order to reduce the execution time during lectures. Please feel free to uncomment the original code and run it in the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 51, Steps: 10, eps: 0.010WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Episode: 99, Steps: 8, eps: 0.0100"
     ]
    }
   ],
   "source": [
    "# for episode in range(600):\n",
    "for episode in range(100):\n",
    "    obs = env.reset()\n",
    "    # for step in range(200):\n",
    "    for step in range(50):\n",
    "        # epsilon = max(1 - episode / 500, 0.01)\n",
    "        epsilon = max(1 - episode / 10, 0.01)\n",
    "        obs, reward, done, info = play_one_step(env, obs, epsilon)\n",
    "        if done:\n",
    "            break\n",
    "    rewards.append(step) # Not shown in the book\n",
    "    if step > best_score: # Not shown\n",
    "        best_weights = model.get_weights() # Not shown\n",
    "        best_score = step # Not shown\n",
    "    print(\"\\rEpisode: {}, Steps: {}, eps: {:.3f}\".format(episode, step + 1, epsilon), end=\"\") # Not shown\n",
    "    if episode > 50:\n",
    "        training_step(batch_size)\n",
    "\n",
    "model.set_weights(best_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cart-pole](images/cart-pole-image3.png \"Cart-Pole\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the algorithm made no apparent progress at all for almost 300 episodes. Then its performance suddenly\n",
    "skyrocketed up to 200. That’s great news: the algorithm worked fine, and it actually ran much faster than the Policy Gradient algorithm!\n",
    "\n",
    "But just a few episodes later, it forgot everything it knew, and its performance dropped below 25! This is called catastrophic forgetting, and it is one of the big problems facing virtually all RL algorithms!\n",
    "\n",
    "Also, we didn't plot the loss since it is a poor indicator of the model’s performance. The loss might go down, yet the agent might perform worse and vice-versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<video controls src=\"videos/cart-pole-video5.mp4\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad at all!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic Deep Q-Learning algorithm we’ve been using so far would be too unstable to learn to play Atari games. So how did DeepMind do it? Well, they tweaked the algorithm!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Go Back to the Slides**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(32, activation=\"elu\", input_shape=[4]),\n",
    "    keras.layers.Dense(32, activation=\"elu\"),\n",
    "    keras.layers.Dense(n_outputs)\n",
    "])\n",
    "\n",
    "target = keras.models.clone_model(model)\n",
    "target.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "discount_rate = 0.95\n",
    "optimizer = keras.optimizers.Adam(lr=1e-3)\n",
    "loss_fn = keras.losses.Huber()\n",
    "\n",
    "def training_step(batch_size):\n",
    "    experiences = sample_experiences(batch_size)\n",
    "    states, actions, rewards, next_states, dones = experiences\n",
    "    next_Q_values = model.predict(next_states)\n",
    "    best_next_actions = np.argmax(next_Q_values, axis=1)\n",
    "    next_mask = tf.one_hot(best_next_actions, n_outputs).numpy()\n",
    "    next_best_Q_values = (target.predict(next_states) * next_mask).sum(axis=1)\n",
    "    target_Q_values = (rewards + \n",
    "                       (1 - dones) * discount_rate * next_best_Q_values)\n",
    "    target_Q_values = target_Q_values.reshape(-1, 1)\n",
    "    mask = tf.one_hot(actions, n_outputs)\n",
    "    with tf.GradientTape() as tape:\n",
    "        all_Q_values = model(states)\n",
    "        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
    "        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_memory = deque(maxlen=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please note:** We have set the number of episode range to 100, and step range to 50 in order to reduce the execution time during lectures. Please feel free to uncomment the original code and run it in the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 51, Steps: 10, eps: 0.010WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Episode: 99, Steps: 8, eps: 0.0100"
     ]
    }
   ],
   "source": [
    "env.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "rewards = []\n",
    "best_score = 0\n",
    "\n",
    "# for episode in range(600):\n",
    "for episode in range(100):\n",
    "    obs = env.reset()\n",
    "    # for step in range(200):\n",
    "    for step in range(50):\n",
    "        # epsilon = max(1 - episode / 500, 0.01)\n",
    "        epsilon = max(1 - episode / 10, 0.01)\n",
    "        obs, reward, done, info = play_one_step(env, obs, epsilon)\n",
    "        if done:\n",
    "            break\n",
    "    rewards.append(step)\n",
    "    if step > best_score:\n",
    "        best_weights = model.get_weights()\n",
    "        best_score = step\n",
    "    print(\"\\rEpisode: {}, Steps: {}, eps: {:.3f}\".format(episode, step + 1, epsilon), end=\"\")\n",
    "    if episode > 50:\n",
    "        training_step(batch_size)\n",
    "    if episode % 50 == 0:\n",
    "        target.set_weights(model.get_weights())\n",
    "    # Alternatively, you can do soft updates at each step:\n",
    "    #if episode > 50:\n",
    "        #target_weights = target.get_weights()\n",
    "        #online_weights = model.get_weights()\n",
    "        #for index in range(len(target_weights)):\n",
    "        #    target_weights[index] = 0.99 * target_weights[index] + 0.01 * online_weights[index]\n",
    "        #target.set_weights(target_weights)\n",
    "\n",
    "model.set_weights(best_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cart-pole](images/cart-pole-image4.png \"Cart-Pole\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "<video controls src=\"videos/cart-pole-video6.mp4\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Go Back to the Slides**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dueling Double DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "K = keras.backend\n",
    "input_states = keras.layers.Input(shape=[4])\n",
    "hidden1 = keras.layers.Dense(32, activation=\"elu\")(input_states)\n",
    "hidden2 = keras.layers.Dense(32, activation=\"elu\")(hidden1)\n",
    "state_values = keras.layers.Dense(1)(hidden2)\n",
    "raw_advantages = keras.layers.Dense(n_outputs)(hidden2)\n",
    "advantages = raw_advantages - K.max(raw_advantages, axis=1, keepdims=True)\n",
    "Q_values = state_values + advantages\n",
    "model = keras.models.Model(inputs=[input_states], outputs=[Q_values])\n",
    "\n",
    "target = keras.models.clone_model(model)\n",
    "target.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "discount_rate = 0.95\n",
    "optimizer = keras.optimizers.Adam(lr=1e-2)\n",
    "loss_fn = keras.losses.Huber()\n",
    "\n",
    "def training_step(batch_size):\n",
    "    experiences = sample_experiences(batch_size)\n",
    "    states, actions, rewards, next_states, dones = experiences\n",
    "    next_Q_values = model.predict(next_states)\n",
    "    best_next_actions = np.argmax(next_Q_values, axis=1)\n",
    "    next_mask = tf.one_hot(best_next_actions, n_outputs).numpy()\n",
    "    next_best_Q_values = (target.predict(next_states) * next_mask).sum(axis=1)\n",
    "    target_Q_values = (rewards + \n",
    "                       (1 - dones) * discount_rate * next_best_Q_values)\n",
    "    target_Q_values = target_Q_values.reshape(-1, 1)\n",
    "    mask = tf.one_hot(actions, n_outputs)\n",
    "    with tf.GradientTape() as tape:\n",
    "        all_Q_values = model(states)\n",
    "        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
    "        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_memory = deque(maxlen=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please note:** We have set the number of episode range to 100, and step range to 50 in order to reduce the execution time during lectures. Please feel free to uncomment the original code and run it in the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 51, Steps: 9, eps: 0.0100WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Episode: 99, Steps: 12, eps: 0.010"
     ]
    }
   ],
   "source": [
    "env.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "rewards = []\n",
    "best_score = 0\n",
    "\n",
    "# for episode in range(600):\n",
    "for episode in range(100):\n",
    "    obs = env.reset()\n",
    "    # for step in range(200):\n",
    "    for step in range(50):\n",
    "        # epsilon = max(1 - episode / 500, 0.01)\n",
    "        epsilon = max(1 - episode / 10, 0.01)\n",
    "        obs, reward, done, info = play_one_step(env, obs, epsilon)\n",
    "        if done:\n",
    "            break\n",
    "    rewards.append(step)\n",
    "    if step > best_score:\n",
    "        best_weights = model.get_weights()\n",
    "        best_score = step\n",
    "    print(\"\\rEpisode: {}, Steps: {}, eps: {:.3f}\".format(episode, step + 1, epsilon), end=\"\")\n",
    "    if episode > 50:\n",
    "        training_step(batch_size)\n",
    "    if episode % 200 == 0:\n",
    "        target.set_weights(model.get_weights())\n",
    "\n",
    "model.set_weights(best_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cart-pole](images/cart-pole-image5.png \"Cart-Pole\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "<video controls src=\"videos/cart-pole-video7.mp4\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like a pretty robust agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Go Back to the Slides**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using TF-Agents to Beat Breakout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use TF-Agents to create an agent that will learn to play Breakout. We will use the Deep Q-Learning algorithm, so you can easily compare the components with the previous implementation, but TF-Agents implements many other (and more sophisticated) algorithms!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-Agents Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf_agents.environments.wrappers.TimeLimit at 0x7f9b90056080>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tf_agents.environments import suite_gym\n",
    "\n",
    "env = suite_gym.load(\"Breakout-v4\")\n",
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gym.envs.atari.atari_env.AtariEnv at 0x7f9bb8175400>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(step_type=array(0, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([[[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]]], dtype=uint8))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.seed(42)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(step_type=array(1, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([[[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]]], dtype=uint8))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(1) # Fire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cart-pole](images/breakout-plot-image1.png \"Cart-Pole\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(step_type=array(1, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([[[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]]], dtype=uint8))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.current_time_step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedArraySpec(shape=(210, 160, 3), dtype=dtype('uint8'), name='observation', minimum=0, maximum=255)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedArraySpec(shape=(), dtype=dtype('int64'), name='action', minimum=0, maximum=3)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(step_type=ArraySpec(shape=(), dtype=dtype('int32'), name='step_type'), reward=ArraySpec(shape=(), dtype=dtype('float32'), name='reward'), discount=BoundedArraySpec(shape=(), dtype=dtype('float32'), name='discount', minimum=0.0, maximum=1.0), observation=BoundedArraySpec(shape=(210, 160, 3), dtype=dtype('uint8'), name='observation', minimum=0, maximum=255))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.time_step_spec()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Wrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can wrap a TF-Agents environments in a TF-Agents wrapper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf_agents.environments.wrappers.ActionRepeat at 0x7f9b983bb2b0>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tf_agents.environments.wrappers import ActionRepeat\n",
    "\n",
    "repeating_env = ActionRepeat(env, times=4)\n",
    "repeating_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gym.envs.atari.atari_env.AtariEnv at 0x7f9bb8175400>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repeating_env.unwrapped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the list of available wrappers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ActionClipWrapper           Wraps an environment and clips actions to spec before applying.\n",
      "ActionDiscretizeWrapper     Wraps an environment with continuous actions and discretizes them.\n",
      "ActionOffsetWrapper         Offsets actions to be zero-based.\n",
      "ActionRepeat                Repeates actions over n-steps while acummulating the received reward.\n",
      "FlattenObservationsWrapper  Wraps an environment and flattens nested multi-dimensional observations.\n",
      "GoalReplayEnvWrapper        Adds a goal to the observation, used for HER (Hindsight Experience Replay).\n",
      "HistoryWrapper              Adds observation and action history to the environment's observations.\n",
      "OneHotActionWrapper         Converts discrete action to one_hot format.\n",
      "PerformanceProfiler         End episodes after specified number of steps.\n",
      "PyEnvironmentBaseWrapper    PyEnvironment wrapper forwards calls to the given environment.\n",
      "RunStats                    Wrapper that accumulates run statistics as the environment iterates.\n",
      "TimeLimit                   End episodes after specified number of steps.\n"
     ]
    }
   ],
   "source": [
    "import tf_agents.environments.wrappers\n",
    "\n",
    "for name in dir(tf_agents.environments.wrappers):\n",
    "    obj = getattr(tf_agents.environments.wrappers, name)\n",
    "    if hasattr(obj, \"__base__\") and issubclass(obj, tf_agents.environments.wrappers.PyEnvironmentBaseWrapper):\n",
    "        print(\"{:27s} {}\".format(name, obj.__doc__.split(\"\\n\")[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ActionClipWrapper` = Clips the actions to the action spec.\n",
    "\n",
    "`ActionDiscretizeWrapper` = Quantizes a continuous action space to a discrete action space.\n",
    "\n",
    "`ActionRepeat` = Repeats each action over n steps, while accumulating the rewards. In many environments, this can speed up training significantly.\n",
    "\n",
    "`RunStats` = Records environment statistics such as the number of steps and the number of episodes.\n",
    "\n",
    "`TimeLimit` = Interrupts the environment if it runs for longer than a maximum number of steps.\n",
    "\n",
    "`VideoWrapper` = Records a video of the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `suite_gym.load()` function can create an env and wrap it for you, both with TF-Agents environment wrappers and Gym environment wrappers (the latter are applied first)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from gym.wrappers import TimeLimit\n",
    "\n",
    "limited_repeating_env = suite_gym.load(\n",
    "    \"Breakout-v4\",\n",
    "    gym_env_wrappers=[partial(TimeLimit, max_episode_steps=10000)],\n",
    "    env_wrappers=[partial(ActionRepeat, times=4)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the list of preprocessing steps `AtariPreprocessing` wrapper supports:\n",
    "\n",
    "Grayscale and downsampling = Observations are converted to grayscale and downsampled.\n",
    "\n",
    "Max pooling = The last two frames of the game are max-pooled using a 1 × 1 filter to remove the flickering.\n",
    "\n",
    "Frame skipping = The agent only gets to see every n frames of the game (by default n = 4), and its actions are repeated for each frame, collecting all the rewards.\n",
    "\n",
    "End on life lost = In some games, the rewards are just based on the score, so the agent gets no immediate penalty for losing a life. One solution is to end the game immediately whenever a life is lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf_agents.environments.wrappers.ActionRepeat at 0x7f9bb847d940>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "limited_repeating_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an Atari Breakout environment, and wrap it to apply the default Atari preprocessing steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gym.envs.atari.atari_env.AtariEnv at 0x7f9bb8175cc0>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "limited_repeating_env.unwrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.environments import suite_atari\n",
    "from tf_agents.environments.atari_preprocessing import AtariPreprocessing\n",
    "from tf_agents.environments.atari_wrappers import FrameStack4\n",
    "\n",
    "max_episode_steps = 27000 # <=> 108k ALE frames since 1 step = 4 frames\n",
    "environment_name = \"BreakoutNoFrameskip-v4\"\n",
    "\n",
    "env = suite_atari.load(\n",
    "    environment_name,\n",
    "    max_episode_steps=max_episode_steps,\n",
    "    gym_env_wrappers=[AtariPreprocessing, FrameStack4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf_agents.environments.atari_wrappers.AtariTimeLimit at 0x7f9c3382ef98>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play a few steps just to see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.seed(42)\n",
    "env.reset()\n",
    "time_step = env.step(1) # FIRE\n",
    "for _ in range(4):\n",
    "    time_step = env.step(3) # LEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_observation(obs):\n",
    "    # Since there are only 3 color channels, you cannot display 4 frames\n",
    "    # with one primary color per frame. So this code computes the delta between\n",
    "    # the current frame and the mean of the other frames, and it adds this delta\n",
    "    # to the red and blue channels to get a pink color for the current frame.\n",
    "    obs = obs.astype(np.float32)\n",
    "    img = obs[..., :3]\n",
    "    current_frame_delta = np.maximum(obs[..., 3] - obs[..., :3].mean(axis=-1), 0.)\n",
    "    img[..., 0] += current_frame_delta\n",
    "    img[..., 2] += current_frame_delta\n",
    "    img = np.clip(img / 150, 0, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cart-pole](images/breakout-plot-image2.png \"Cart-Pole\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this single observation, the agent can see that the ball is going toward the lower-left corner, and that it should continue to move the paddle to the left."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the Python environment to a TF environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.environments.tf_py_environment import TFPyEnvironment\n",
    "\n",
    "tf_env = TFPyEnvironment(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Go Back to the Slides**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a small class to normalize the observations. Images are stored using bytes from 0 to 255 to use less RAM, but we want to pass floats from 0.0 to 1.0 to the neural network:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Q-Network:\n",
    "\n",
    "This QNetwork takes an observation as input and outputs one Q-Value per action, so we must give it the specifications of the observations and the actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.networks.q_network import QNetwork\n",
    "\n",
    "# Casts the observations to 32-bit floats and normalizes them (the values will range from 0.0 to 1.0)\n",
    "preprocessing_layer = keras.layers.Lambda(\n",
    "                          lambda obs: tf.cast(obs, np.float32) / 255.)\n",
    "\n",
    "# Encoding network will optionally apply a list of convolutions sequentially\n",
    "# provided you specify their parameters via the next layer\n",
    "conv_layer_params=[(32, (8, 8), 4), (64, (4, 4), 2), (64, (3, 3), 1)]\n",
    "\n",
    "# After these convolutional layers, the encoding network will optionally apply\n",
    "# a sequence of dense layers, if you set the next layer fc_layer_params argument\n",
    "fc_layer_params=[512]\n",
    "\n",
    "q_net = QNetwork(\n",
    "    tf_env.observation_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    preprocessing_layers=preprocessing_layer,\n",
    "    conv_layer_params=conv_layer_params,\n",
    "    fc_layer_params=fc_layer_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cart-pole](images/encoding-network.png \"Cart-Pole\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the DQN Agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.agents.dqn.dqn_agent import DqnAgent\n",
    "\n",
    "# see TF-agents issue #113\n",
    "#optimizer = keras.optimizers.RMSprop(lr=2.5e-4, rho=0.95, momentum=0.0,\n",
    "#                                     epsilon=0.00001, centered=True)\n",
    "\n",
    "train_step = tf.Variable(0)\n",
    "update_period = 4 # run a training step every 4 collect steps\n",
    "optimizer = tf.compat.v1.train.RMSPropOptimizer(learning_rate=2.5e-4, decay=0.95, momentum=0.0,\n",
    "                                     epsilon=0.00001, centered=True)\n",
    "\n",
    "# Compute the ε value for the ε-greedy collect policy, given the current training step\n",
    "epsilon_fn = keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate=1.0, # initial ε\n",
    "    decay_steps=250000 // update_period, # <=> 1,000,000 ALE frames\n",
    "    end_learning_rate=0.01) # final ε\n",
    "agent = DqnAgent(tf_env.time_step_spec(),\n",
    "                 tf_env.action_spec(),\n",
    "                 q_network=q_net,\n",
    "                 optimizer=optimizer,\n",
    "                 target_update_period=2000, # <=> 32,000 ALE frames\n",
    "                 td_errors_loss_fn=keras.losses.Huber(reduction=\"none\"),\n",
    "                 gamma=0.99, # discount factor\n",
    "                 train_step_counter=train_step,\n",
    "                 epsilon_greedy=lambda: epsilon_fn(train_step))\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TF-Agents library provides various replay buffer implementations in the `tf_agents.replay_buffers` package. We will use the `TFUniformReplayBuffer` class in the `tf_agents.replay_buffers.tf_uniform_replay_buffer` package. It provides a high-performance implementation of a replay buffer with uniform sampling:\n",
    "\n",
    "**Plase note:** We have changed the buffer size to 1000 to suit our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    # Specification of the data that will be saved in the replay buffer\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    # Number of trajectories that will be added at each step\n",
    "    batch_size=tf_env.batch_size,\n",
    "    # The maximum size of the replay buffer\n",
    "    # max_length=1000000\n",
    "    max_length=1000)\n",
    "\n",
    "replay_buffer_observer = replay_buffer.add_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a simple custom observer that counts and displays the number of times it is called (except when it is passed a trajectory that represents the boundary between two episodes, as this does not count as a step):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShowProgress:\n",
    "    def __init__(self, total):\n",
    "        self.counter = 0\n",
    "        self.total = total\n",
    "    def __call__(self, trajectory):\n",
    "        if not trajectory.is_boundary():\n",
    "            self.counter += 1\n",
    "        if self.counter % 100 == 0:\n",
    "            print(\"\\r{}/{}\".format(self.counter, self.total), end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-Agents implements several RL metrics in the `tf_agents.metrics` package, some purely in Python and some based on TensorFlow. Let's add some training metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.metrics import tf_metrics\n",
    "\n",
    "train_metrics = [\n",
    "    tf_metrics.NumberOfEpisodes(),\n",
    "    tf_metrics.EnvironmentSteps(),\n",
    "    tf_metrics.AverageReturnMetric(),\n",
    "    tf_metrics.AverageEpisodeLengthMetric(),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int64, numpy=0>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_metrics[0].result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 0\n",
      "\t\t EnvironmentSteps = 0\n",
      "\t\t AverageReturn = 0.0\n",
      "\t\t AverageEpisodeLength = 0.0\n"
     ]
    }
   ],
   "source": [
    "from tf_agents.eval.metric_utils import log_metrics\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "log_metrics(train_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the collect driver:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver\n",
    "\n",
    "collect_driver = DynamicStepDriver(\n",
    "    tf_env,\n",
    "    agent.collect_policy,\n",
    "    observers=[replay_buffer_observer] + train_metrics,\n",
    "    num_steps=update_period) # collect 4 steps for each training iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two main driver classes: `DynamicStepDriver` and `DynamicEpisodeDriver`. The first one collects experiences for a given number of steps, while the second collects experiences for a given number of episodes. We want to collect experiences for four steps for each training iteration (as was done in the 2015 DQN paper), so let’s create a `DynamicStepDriver` and collect the initial experiences, before training:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please note:** We have changed the number of steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/20000"
     ]
    }
   ],
   "source": [
    "from tf_agents.policies.random_tf_policy import RandomTFPolicy\n",
    "\n",
    "initial_collect_policy = RandomTFPolicy(tf_env.time_step_spec(),\n",
    "                                        tf_env.action_spec())\n",
    "init_driver = DynamicStepDriver(\n",
    "    tf_env,\n",
    "    initial_collect_policy,\n",
    "    observers=[replay_buffer.add_batch, ShowProgress(20000)],\n",
    "    num_steps=2000)\n",
    "    # num_steps=20000) # <=> 80,000 ALE frames\n",
    "final_time_step, final_policy_state = init_driver.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will sample a small batch of two trajectories (subepisodes), each containing three consecutive steps. Let's sample 2 sub-episodes, with 3 time steps each and display them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(888) # chosen to show an example of trajectory at the end of an episode\n",
    "\n",
    "trajectories, buffer_info = replay_buffer.get_next(\n",
    "    sample_batch_size=2, num_steps=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('step_type',\n",
       " 'observation',\n",
       " 'action',\n",
       " 'policy_info',\n",
       " 'next_step_type',\n",
       " 'reward',\n",
       " 'discount')"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajectories._fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 3, 84, 84, 4])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajectories.observation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 2, 84, 84, 4])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tf_agents.trajectories.trajectory import to_transition\n",
    "\n",
    "time_steps, action_steps, next_time_steps = to_transition(trajectories)\n",
    "time_steps.observation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1],\n",
       "       [1, 1, 1]], dtype=int32)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajectories.step_type.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x489.6 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6.8))\n",
    "for row in range(2):\n",
    "    for col in range(3):\n",
    "        plt.subplot(2, 3, row * 3 + col + 1)\n",
    "        plot_observation(trajectories.observation[row, col].numpy())\n",
    "plt.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0, wspace=0.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each trajectory is a concise representation of a sequence of consecutive time steps and action steps, designed to avoid redundancy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cart-pole](images/trajectories.png \"Cart-Pole\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = replay_buffer.as_dataset(\n",
    "    sample_batch_size=64,\n",
    "    num_steps=2,\n",
    "    num_parallel_calls=3).prefetch(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the main functions to TF Functions for better performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.utils.common import function\n",
    "\n",
    "collect_driver.run = function(collect_driver.run)\n",
    "agent.train = function(agent.train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we are ready to run the main loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(n_iterations):\n",
    "    time_step = None\n",
    "    policy_state = agent.collect_policy.get_initial_state(tf_env.batch_size)\n",
    "    iterator = iter(dataset)\n",
    "    for iteration in range(n_iterations):\n",
    "        time_step, policy_state = collect_driver.run(time_step, policy_state)\n",
    "        trajectories, buffer_info = next(iterator)\n",
    "        train_loss = agent.train(trajectories)\n",
    "        print(\"\\r{} loss:{:.5f}\".format(\n",
    "            iteration, train_loss.loss.numpy()), end=\"\")\n",
    "        if iteration % 1000 == 0:\n",
    "            log_metrics(train_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next cell to train the agent for 10,000 steps. Then look at its behavior by running the following cell. You can run these two cells as many times as you wish. The agent will keep improving!\n",
    "\n",
    "**Please note:** We have changed the number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 0\n",
      "\t\t EnvironmentSteps = 4\n",
      "\t\t AverageReturn = 0.0\n",
      "\t\t AverageEpisodeLength = 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 loss:0.00286"
     ]
    }
   ],
   "source": [
    "# train_agent(n_iterations=10000)\n",
    "train_agent(n_iterations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000"
     ]
    }
   ],
   "source": [
    "frames = []\n",
    "def save_frames(trajectory):\n",
    "    global frames\n",
    "    frames.append(tf_env.pyenv.envs[0].render(mode=\"rgb_array\"))\n",
    "\n",
    "prev_lives = tf_env.pyenv.envs[0].ale.lives()\n",
    "def reset_and_fire_on_life_lost(trajectory):\n",
    "    global prev_lives\n",
    "    lives = tf_env.pyenv.envs[0].ale.lives()\n",
    "    if prev_lives != lives:\n",
    "        tf_env.reset()\n",
    "        tf_env.pyenv.envs[0].step(1)\n",
    "        prev_lives = lives\n",
    "\n",
    "watch_driver = DynamicStepDriver(\n",
    "    tf_env,\n",
    "    agent.policy,\n",
    "    observers=[save_frames, reset_and_fire_on_life_lost, ShowProgress(1000)],\n",
    "    num_steps=1000)\n",
    "final_time_step, final_policy_state = watch_driver.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<video controls src=\"videos/breakout-plot-video1.mp4\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Go Back to the Slides**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow2",
   "language": "python",
   "name": "tensorflow2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
